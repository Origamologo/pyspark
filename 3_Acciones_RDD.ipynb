{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/17 14:15:08 WARN Utils: Your hostname, RNTDELL000700 resolves to a loopback address: 127.0.1.1; using 192.168.1.131 instead (on interface wlp0s20f3)\n",
      "23/07/17 14:15:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/17 14:15:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Acciones en un RDD\n",
    "\n",
    "Recordemos que las acciones son operaciones que activan los cálculos.\\\n",
    "Hasta que se encuentra una operación de acción el plan de ejecución dentro del programa Spark se crea en forma de DAG, por tanto, no se hará nada hasta que se realice una acción dentro del DAG. Podría haber varias transformaciones de todo tipo dentro del plan de ejecución, pero no sucede nada hasta que realicemos una acción.\\\n",
    "Cualquier tipo de variables que nosotros podamos tener en memoria acelerará grandemente nuestras aplicaciones de Spark, debido a que no se tendrá que ejecutar todo el DAG para reconstruir el RDD que querramos en ese momento.\n",
    "\n",
    "\n",
    "#### Tipos de acciones\n",
    "La acción activa todo el tag de las transformaciones construidas hasta ese momento, para que se materialicen ejecutando los bloques de código y las funciones. Existen dos tipos de operaciones de acción: \n",
    "\n",
    "   * **Las que ocurren en el driver:** suelen recopilar conteos, recuentos por tecla, etc. Cada una de estas acciones realiza algunos cálculos en el ejecutor remoto (cluster nodes) y devuelve los datos al driver.\n",
    "\n",
    "   * **Las distribuidas:** Son los que se ejecutan en los nodos del cluster. Un ejemplo de tal acción distribuida es saveAsTextFile(); esta es la operación de acción más común debido a la naturaleza distribuida deseable de la misma operación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Función reduce\n",
    "\n",
    "Reduce aplica la función de reducción a todos los elementos del RDD y la envía al driver.\\\n",
    "Se aplica reduce con una función lambda, en el RDD se aplica la función de reducción a cada uno de los elementos del RDD y se envía la información al driver.\\\n",
    "Debemos tener en cuenta que al ser una acción, reduce automáticamente va a desencadenar el DAG y por lo tanto nos va a devolver un resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([2,4,6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sumamos todos los elementos del RDD\n",
    "rdd.reduce(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un RDD y multiplicamos sus elementos entre sí\n",
    "rdd1 = sc.parallelize([1,2,3,4])\n",
    "\n",
    "rdd1.reduce(lambda x,y: x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Función count\n",
    "\n",
    "Cuenta el número de elementos del RDD y lo envía al controlador.\\\n",
    "El driver le pide a cada ejecutor que cuente la cantidad de elementos en la partición que está siendo manejada por la tarea y luego suma los recuentos de todas las tareas juntas en el nivel del driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([item for item in range(10)])\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Función collect\n",
    "\n",
    "Recopila todos los elementos del RDD y lo envía al driver. El driver extrae todos los elementos del RDD de todas las particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'Apache', 'Spark!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_frase = sc.parallelize('Hola Apache Spark!'.split(' '))\n",
    "rdd_frase.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 4),\n",
       " (3, 9),\n",
       " (4, 16),\n",
       " (5, 25),\n",
       " (6, 36),\n",
       " (7, 49),\n",
       " (8, 64),\n",
       " (9, 81),\n",
       " (10, 100),\n",
       " (11, 121),\n",
       " (12, 144),\n",
       " (13, 169),\n",
       " (14, 196),\n",
       " (15, 225),\n",
       " (16, 256),\n",
       " (17, 289),\n",
       " (18, 324),\n",
       " (19, 361)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([(item, item ** 2) for item in range(20)])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Funciones take, max y saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_frase2 = sc.parallelize('La programación es bella'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La', 'programación']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take recibe un entero N y nos devuelve los primero N elementos que encuentre dentro del RDD\n",
    "rdd_frase2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La', 'programación', 'es', 'bella']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_frase2.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max nos va a devolver el máximo de los números que encuentren un RD que esté construido a partir de números.\n",
    "rdd3 = sc.parallelize([item/(item + 1) for item in range(10)])\n",
    "rdd3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.5,\n",
       " 0.6666666666666666,\n",
       " 0.75,\n",
       " 0.8,\n",
       " 0.8333333333333334,\n",
       " 0.8571428571428571,\n",
       " 0.875,\n",
       " 0.8888888888888888,\n",
       " 0.9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAsTextFile guarda los elementos de un RDD en un fichero de texto\n",
    "# Esto nos lo va a guardar en un montón de ficheros\n",
    "rdd_frase2.saveAsTextFile('./rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que lo guarde en un solo fichero, combinamos saveAsTexFile con coalesce\n",
    "rdd_frase2.coalesce(1).saveAsTextFile('./rdd1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Si los ficheros no son muy grandes, comprimirlos en una sola partición puede facilitar las acciones que hagamos sobre ellos\n",
    ">\n",
    "><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
