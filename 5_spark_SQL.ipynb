{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "* En Spark 1.6 se introdujo una nueva abstracción de programación llamada API estructurada. Esta es la forma preferida para realizar el procesamiento de datos en la mayoría de los casos de uso.\n",
    "\n",
    "* En esta nueva forma de hacer el procesamiento de datos, los datos deben organizarse en un formato estructurado y la lógica de cálculo de datos debe seguir una determinada estructura. Con estas dos piezas de información, Spark puede realizar optimizaciones para acelerar las aplicaciones de procesamiento de datos.\n",
    "\n",
    "* El componente SparkSQL está construido sobre el viejo y confiable componente SparkCore. Esta arquitectura en capas significa que cualquier mejora en el componente Spark Core estará disponible automáticamente para el componente para SQL.\n",
    "\n",
    "* El concepto DF se inspiró en el concepto de pandasDF de python. La principal diferencia es que un DF en Spark puede manejar un gran volumen de datos que se distribuyen en muchas máquinas.\n",
    "\n",
    "* Un concepto fundamental que diferencia a los datos estructurados de los no estructurados es un **esquema** que define la estructura de los datos en forma de nombres de columna y tipos de datos asociados. El concepto de **esquema** es una parte integral de las APIs estructuradas de Spark.\n",
    "\n",
    "* Los datos estructurados a menudo se capturan en un formato determinado. Algunos de estos formatos están basados en textos y algunos de ellos están basados en binario.\n",
    "\n",
    "    * Los formatos comunes para datos de texto son CSV, XML y JSON \n",
    "    * Los formatos comunes para datos binarios son agro, parquet y ORC.\n",
    "<br>.\n",
    "* El módulo SparkSQL y facilita la lectura y escritura de datos desde y hacia cualquiera de estos formatos. Un beneficio inesperado que surge de esta versatilidad es que Spark se puede utilizar como una herramienta de conversión de formato de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un DF a partir de un RDD\n",
    "\n",
    "Hay muchas formas de crear un DF, pero **siempre hay que proporcionar un esquema**, ya sea implícita o explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 4),\n",
       " (3, 9),\n",
       " (4, 16),\n",
       " (5, 25),\n",
       " (6, 36),\n",
       " (7, 49),\n",
       " (8, 64),\n",
       " (9, 81)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numero: long (nullable = true)\n",
      " |-- cuadrado: long (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|numero|cuadrado|\n",
      "+------+--------+\n",
      "|     0|       0|\n",
      "|     1|       1|\n",
      "|     2|       4|\n",
      "|     3|       9|\n",
      "|     4|      16|\n",
      "|     5|      25|\n",
      "|     6|      36|\n",
      "|     7|      49|\n",
      "|     8|      64|\n",
      "|     9|      81|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para crear un DF a parti de un RDD no tenemos más que pasarle a la función .toDF los nombres de las columnas como lista\n",
    "df = rdd.toDF(['numero', 'cuadrado'])\n",
    "\n",
    "# Podemos ver el esquema de los datos, que nos informa del tipo de datos y de si acepta nulos o no\n",
    "df.printSchema()\n",
    "\n",
    "# Podemos ver el DF\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### También podemos crear un DF a partir de un RDD definiendo nosotros el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 1: creamos el esquema a partir de las classes de pyspark.sql.types\n",
    "\n",
    "esquema1 = StructType(\n",
    "    [\n",
    "     StructField('id', IntegerType(), True),\n",
    "     StructField('nombre', StringType(), True),\n",
    "     StructField('saldo', DoubleType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 2: creamos el esquema a partir de un string\n",
    "\n",
    "esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- saldo: double (nullable = true)\n",
      "\n",
      "+---+------+-----+\n",
      "| id|nombre|saldo|\n",
      "+---+------+-----+\n",
      "|  1|  Jose| 35.5|\n",
      "|  2|Teresa| 54.3|\n",
      "|  3| Katia| 12.7|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finalmente, para crear un DF aplicando nuestro esquema, usamos la función .createDtaFrame, pasándole el RDD y el esquema\n",
    "\n",
    "df1 = spark.createDataFrame(rdd1, schema=esquema1)\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- saldo: double (nullable = true)\n",
      "\n",
      "+---+------+-----+\n",
      "| id|nombre|saldo|\n",
      "+---+------+-----+\n",
      "|  1|  Jose| 35.5|\n",
      "|  2|Teresa| 54.3|\n",
      "|  3| Katia| 12.7|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(rdd1, schema=esquema2)\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un DF a partir de fuentes de datos\n",
    "\n",
    "Las dos clases principales en SparkSQL para leer y escribir datos son DataFrameReader y DataFrameWriter respectivamente.\n",
    "\n",
    "#### DataFrameReader\n",
    "\n",
    "* Una instancia de la clase DataFrameReader está disponible como read en la sesión de Spark y la podemos invocar a través de **spark.read**\n",
    "\n",
    "* El patrón común para interactuar con DataFrameReader es **spark.read.format(...).option('key', 'value').schema(...).load()**\n",
    "\n",
    "    * **.format(...)** no es opcional. Puede ser una de las fuente de datos integradas o un formato de dato personalizado\n",
    "\n",
    "        * **Formato integrado:** se puede usar un nombre corto como por ejemplo json, parquet, jdbc, orc, csv, text...\n",
    "        * **Formato personalizado:** debe proporcionar un nombre completo\n",
    "<br>.\n",
    "\n",
    "    * **.option('key', 'value')** es opcional porque DataFrameReader tiene un conjunto de opciones predeterminadas para cada formato de fuente de datos. Podemos anular estos valores predeterminados proporcionando un valor a la función option('key', 'value')\n",
    "\n",
    "    * **.schema(...)** puede ser opcional porque algunas fuentes de datos tienen el esquema incrustado dentro de los archivos de datos, podemos pensar en .parquet u .orc En estos casos el esquema se infiere automáticamente, para otros casos es posible que deba proporcionar un esquema.\n",
    "\n",
    "* Para leer los datos hay dos alternativas aplicables a todos los formatos:\n",
    "\n",
    "    * **spark.read.<extension>(\"<path>\")** por ejemplo, spark.read.json(\"/path/to/file.json\")\n",
    "    * **spark.read.format(\"<extension>\").load(\"<path>\")** por ejemplo spark.read.format(\"json\").load(\"/path/to/file.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|value                                                                  |\n",
      "+-----------------------------------------------------------------------+\n",
      "|Estamos en el curso de pyspark                                         |\n",
      "|En este capítulo estamos estudiando el API SQL de Saprk                |\n",
      "|En esta sección estamos creado dataframes a partir de fuentes de datos,|\n",
      "|y en este ejemplo creamos un dataframe a partir de un texto plano      |\n",
      "+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DF a partir de un archivo de texto\n",
    "dftxt = spark.read.text('./data/data_DF/dataTXT.txt')\n",
    "\n",
    "dftxt.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+--------------------+--------------------+\n",
      "|        _c0|          _c1|                 _c2|                 _c3|        _c4|                 _c5|                 _c6|    _c7|   _c8|     _c9|         _c10|                _c11|             _c12|            _c13|                _c14|                _c15|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+--------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_re...|         description|\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|               False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|               False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|               False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|               False|Today we find out...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame mediante la lectura de un archivo csv\n",
    "\n",
    "dfcsv = spark.read.csv('./data/data_DF/dataCSV.csv')\n",
    "\n",
    "# Aquí mete el nombre de las columnas en la primera fila y da nombres nuevos predeterminados\n",
    "dfcsv.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
      "|d380meD0W0M|     17.14.11|I Dare You: GOING...|            nigahiga|         24|2017-11-12T18:01:...|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para que tome la primera fila como nombre de columnas:\n",
    "dfcsv1 = spark.read.option('header', 'true').csv('./data/data_DF/dataCSV.csv')\n",
    "\n",
    "dfcsv1.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-----+\n",
      "|pais|edad|     fecha|color|\n",
      "+----+----+----------+-----+\n",
      "|  MX|  23|2021-02-21| rojo|\n",
      "|  CA|  56|2021-06-10| azul|\n",
      "|  US|  32|2020-06-02|verde|\n",
      "+----+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leer un archivo de texto con un delimitador diferente\n",
    "\n",
    "dftxt1 = spark.read.option('header', 'true').option('delimiter', '|').csv('./data/data_DF/dataTab.txt')\n",
    "\n",
    "dftxt1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+----+\n",
      "|color|edad|     fecha|pais|\n",
      "+-----+----+----------+----+\n",
      "| rojo|null|2021-02-21|  MX|\n",
      "| azul|null|2021-06-10|  CA|\n",
      "|verde|null|2020-06-02|  US|\n",
      "+-----+----+----------+----+\n",
      "\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un json proporcionando un schema\n",
    "json_schema =  StructType(\n",
    "    [\n",
    "     StructField('color', StringType(), True),\n",
    "     StructField('edad', IntegerType(), True),\n",
    "     StructField('fecha', DateType(), True),\n",
    "     StructField('pais', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dfjson = spark.read.schema(json_schema).json('./data/data_DF/dataJSON.json')\n",
    "\n",
    "dfjson.show()\n",
    "\n",
    "dfjson.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
      "|d380meD0W0M|     17.14.11|I Dare You: GOING...|            nigahiga|         24|2017-11-12T18:01:...|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un archivo parquet\n",
    "dfparquet = spark.read.parquet('./data/data_DF/dataPARQUET.parquet')\n",
    "\n",
    "dfparquet.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Otra alternativa para leer desde una fuente de datos parquet en este caso\n",
    "dfparquet1 = spark.read.format('parquet').load('./data/data_DF/dataPARQUET.parquet')\n",
    "\n",
    "dfparquet1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajo con columnas\n",
    "\n",
    "* A diferencia de las operaciones con RDD, las operaciones estructuradas están diseñadas para ser más relacionales, lo que significa que estas operaciones reflejan el tipo de expresiones que pueden hacer con SQL como filtrado, transformación, unión, entre otras.\n",
    "\n",
    "* Al igual que las operaciones con RDD, las operaciones estructuradas se dividen en dos categorías: transformaciones y acciones. La semántica de las transformaciones y acciones estructuradas es idéntica a la de los RDD: las transformaciones estructuradas son **lazy evaluation** y las acciones estructuradas son **eager evaluation**.\n",
    "\n",
    "* Recordmeos que los DF son inmutables y sus operaciones de transformación siempre devuelven un nuevo DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet2 = spark.read.parquet('./data/dataPARQUET.parquet')\n",
    "\n",
    "dfparquet2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|WE WANT TO TALK A...|\n",
      "|The Trump Preside...|\n",
      "|Racist Superman |...|\n",
      "|Nickelback Lyrics...|\n",
      "|I Dare You: GOING...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Primera alternativa para referirnos a las columnas\n",
    "\n",
    "dfparquet2.select('title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|WE WANT TO TALK A...|\n",
      "|The Trump Preside...|\n",
      "|Racist Superman |...|\n",
      "|Nickelback Lyrics...|\n",
      "|I Dare You: GOING...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Segunda alternativapara referirnos a las columnas\n",
    "\n",
    "dfparquet2.select(col('title')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones: funciones select y selectExpr\n",
    "\n",
    "* **select:** esta transformación se usa comúnmente para realizar la proyección, es decir, seleccionar todas o un subconjunto de columnas de un DF. Durante la selección cada columna se puede transformar mediante una expresión de columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el parquet, muy similar al anterior, pero con alguna modificación en el tipo de datos de cada columna\n",
    "dfparquet3 = spark.read.parquet('./data/datos.parquet')\n",
    "\n",
    "dfparquet3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   video_id|\n",
      "+-----------+\n",
      "|2kyS6SvSYSE|\n",
      "|1ZAPwfrtAFY|\n",
      "|5qpjK5DgCt4|\n",
      "|puqaWrEC7tY|\n",
      "|d380meD0W0M|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.select(col('video_id')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|   video_id|trending_date|\n",
      "+-----------+-------------+\n",
      "|2kyS6SvSYSE|     17.14.11|\n",
      "|1ZAPwfrtAFY|     17.14.11|\n",
      "|5qpjK5DgCt4|     17.14.11|\n",
      "|puqaWrEC7tY|     17.14.11|\n",
      "|d380meD0W0M|     17.14.11|\n",
      "+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.select('video_id', 'trending_date').show(5)\n",
    "# El problema de esta nomenclatura es que no podemos pasarle funciones, como se puede comprobar en la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Esta expresión nos dará error\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dfparquet3\u001b[39m.\u001b[39mselect(\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlikes\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdislikes\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m----> 6\u001b[0m     (\u001b[39m'\u001b[39;49m\u001b[39mlikes\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdislikes\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m )\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Esta expresión nos dará error\n",
    "\n",
    "dfparquet3.select(\n",
    "    'likes',\n",
    "    'dislikes',\n",
    "    ('likes' - 'dislikes')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "| likes|dislikes|aceptacion|\n",
      "+------+--------+----------+\n",
      "| 57527|    2966|     54561|\n",
      "| 97185|    6146|     91039|\n",
      "|146033|    5339|    140694|\n",
      "| 10172|     666|      9506|\n",
      "|132235|    1989|    130246|\n",
      "+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La forma correcta es usando la función col\n",
    "\n",
    "dfparquet3.select(\n",
    "    col('likes'),\n",
    "    col('dislikes'),\n",
    "    (col('likes') - col('dislikes')).alias('aceptacion')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **selectExpr:** nos permite aplicar una expresión a la selección de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "| likes|dislikes|aceptacion|\n",
      "+------+--------+----------+\n",
      "| 57527|    2966|     54561|\n",
      "| 97185|    6146|     91039|\n",
      "|146033|    5339|    140694|\n",
      "| 10172|     666|      9506|\n",
      "|132235|    1989|    130246|\n",
      "+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Le pedimos los mismo que en la celda anterior\n",
    "dfparquet3.selectExpr('likes', 'dislikes', '(likes - dislikes) as aceptacion').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|videos|\n",
      "+------+\n",
      "|  6837|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contamos el número de valores diferentes dentro de la columna video_id\n",
    "dfparquet3.selectExpr(\"count(distinct(video_id)) as videos\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Como conclusión, deberíamos tener en cuenta que las expresiones SQL son construcciones poderosas y flexibles que permiten expresar la lógica de transformación de columnas de una manera natural, tal como lo podríamos estar pensando nosotros mismos. Podemos expresar expresiones SQL en un formato de cadena y spark las analizará en un árbol lógico que se evalúa en un orden correcto. Además hay que tener en cuenta que la combinación de expresiones SQL y funciones integradas facilita la realización de ciertos análisis de datos que de otro modo requerirían varios pasos.\n",
    ">\n",
    "><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
