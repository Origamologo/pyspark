{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "* En Spark 1.6 se introdujo una nueva abstracción de programación llamada API estructurada. Esta es la forma preferida para realizar el procesamiento de datos en la mayoría de los casos de uso.\n",
    "\n",
    "* En esta nueva forma de hacer el procesamiento de datos, los datos deben organizarse en un formato estructurado y la lógica de cálculo de datos debe seguir una determinada estructura. Con estas dos piezas de información, Spark puede realizar optimizaciones para acelerar las aplicaciones de procesamiento de datos.\n",
    "\n",
    "* El componente SparkSQL está construido sobre el viejo y confiable componente SparkCore. Esta arquitectura en capas significa que cualquier mejora en el componente Spark Core estará disponible automáticamente para el componente para SQL.\n",
    "\n",
    "* El concepto DF se inspiró en el concepto de pandasDF de python. La principal diferencia es que un DF en Spark puede manejar un gran volumen de datos que se distribuyen en muchas máquinas.\n",
    "\n",
    "* Un concepto fundamental que diferencia a los datos estructurados de los no estructurados es un **esquema** que define la estructura de los datos en forma de nombres de columna y tipos de datos asociados. El concepto de **esquema** es una parte integral de las APIs estructuradas de Spark.\n",
    "\n",
    "* Los datos estructurados a menudo se capturan en un formato determinado. Algunos de estos formatos están basados en textos y algunos de ellos están basados en binario.\n",
    "\n",
    "    * Los formatos comunes para datos de texto son CSV, XML y JSON \n",
    "    * Los formatos comunes para datos binarios son agro, parquet y ORC.\n",
    "<br>.\n",
    "* El módulo SparkSQL y facilita la lectura y escritura de datos desde y hacia cualquiera de estos formatos. Un beneficio inesperado que surge de esta versatilidad es que Spark se puede utilizar como una herramienta de conversión de formato de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un DF a partir de un RDD\n",
    "\n",
    "Hay muchas formas de crear un DF, pero **siempre hay que proporcionar un esquema**, ya sea implícita o explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 4),\n",
       " (3, 9),\n",
       " (4, 16),\n",
       " (5, 25),\n",
       " (6, 36),\n",
       " (7, 49),\n",
       " (8, 64),\n",
       " (9, 81)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numero: long (nullable = true)\n",
      " |-- cuadrado: long (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|numero|cuadrado|\n",
      "+------+--------+\n",
      "|     0|       0|\n",
      "|     1|       1|\n",
      "|     2|       4|\n",
      "|     3|       9|\n",
      "|     4|      16|\n",
      "|     5|      25|\n",
      "|     6|      36|\n",
      "|     7|      49|\n",
      "|     8|      64|\n",
      "|     9|      81|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para crear un DF a parti de un RDD no tenemos más que pasarle a la función .toDF los nombres de las columnas como lista\n",
    "df = rdd.toDF(['numero', 'cuadrado'])\n",
    "\n",
    "# Podemos ver el esquema de los datos, que nos informa del tipo de datos y de si acepta nulos o no\n",
    "df.printSchema()\n",
    "\n",
    "# Podemos ver el DF\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### También podemos crear un DF a partir de un RDD definiendo nosotros el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 1: creamos el esquema a partir de las classes de pyspark.sql.types\n",
    "\n",
    "esquema1 = StructType(\n",
    "    [\n",
    "     StructField('id', IntegerType(), True),\n",
    "     StructField('nombre', StringType(), True),\n",
    "     StructField('saldo', DoubleType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 2: creamos el esquema a partir de un string\n",
    "\n",
    "esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- saldo: double (nullable = true)\n",
      "\n",
      "+---+------+-----+\n",
      "| id|nombre|saldo|\n",
      "+---+------+-----+\n",
      "|  1|  Jose| 35.5|\n",
      "|  2|Teresa| 54.3|\n",
      "|  3| Katia| 12.7|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finalmente, para crear un DF aplicando nuestro esquema, usamos la función .createDtaFrame, pasándole el RDD y el esquema\n",
    "\n",
    "df1 = spark.createDataFrame(rdd1, schema=esquema1)\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- saldo: double (nullable = true)\n",
      "\n",
      "+---+------+-----+\n",
      "| id|nombre|saldo|\n",
      "+---+------+-----+\n",
      "|  1|  Jose| 35.5|\n",
      "|  2|Teresa| 54.3|\n",
      "|  3| Katia| 12.7|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(rdd1, schema=esquema2)\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un DF a partir de fuentes de datos\n",
    "\n",
    "Las dos clases principales en SparkSQL para leer y escribir datos son DataFrameReader y DataFrameWriter respectivamente.\n",
    "\n",
    "#### DataFrameReader\n",
    "\n",
    "* Una instancia de la clase DataFrameReader está disponible como read en la sesión de Spark y la podemos invocar a través de **spark.read**\n",
    "\n",
    "* El patrón común para interactuar con DataFrameReader es **spark.read.format(...).option('key', 'value').schema(...).load()**\n",
    "\n",
    "    * **.format(...)** no es opcional. Puede ser una de las fuente de datos integradas o un formato de dato personalizado\n",
    "\n",
    "        * **Formato integrado:** se puede usar un nombre corto como por ejemplo json, parquet, jdbc, orc, csv, text...\n",
    "        * **Formato personalizado:** debe proporcionar un nombre completo\n",
    "<br>.\n",
    "    * **.option('key', 'value')** es opcional porque DataFrameReader tiene un conjunto de opciones predeterminadas para cada formato de fuente de datos. Podemos anular estos valores predeterminados proporcionando un valor a la función option('key', 'value')\n",
    "\n",
    "    * **.schema(...)** puede ser opcional porque algunas fuentes de datos tienen el esquema incrustado dentro de los archivos de datos, podemos pensar en .parquet u .orc En estos casos el esquema se infiere automáticamente, para otros casos es posible que deba proporcionar un esquema.\n",
    "\n",
    "* Para leer los datos hay dos alternativas aplicables a todos los formatos:\n",
    "\n",
    "    * **spark.read.<extension>(\"<path>\")** por ejemplo, spark.read.json(\"/path/to/file.json\")\n",
    "    * **spark.read.format(\"<extension>\").load(\"<path>\")** por ejemplo spark.read.format(\"json\").load(\"/path/to/file.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
