{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import count, countDistinct, approx_count_distinct\n",
    "from pyspark.sql.functions import min, max, col\n",
    "from pyspark.sql.functions import sum, sum_distinct, avg\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agregaciones\n",
    "\n",
    "La realización de análisis interesante sobre Big Data generalmente implica algún tipo de agregación para resumir los datos con el fin de extraer patrones, conocimientos o simplemente generar informes resumidos.\n",
    "\n",
    "Las agregaciones generalmente requieren de alguna forma de agrupación, ya sea en todo el conjunto de datos o en una o más columnas, y luego aplican funciones de agregación como sumar, contar o promediar a cada grupo.\n",
    "\n",
    "Spark proporciona muchas funciones de agregación de uso común, así como la capacidad de agregar los valores de una colección que luego se pueden analizar más a fondo.\n",
    "\n",
    "La agrupación de filas se puede realizar en diferentes niveles y spark admite los siguientes niveles:\n",
    "\n",
    "* Tratar un DF como un grupo.\n",
    "\n",
    "* Dividir un DF en varios grupos utilizando una o más columnas y realizar una o más agregaciones en cada uno de estos grupos.\n",
    "\n",
    "* Dividir un DF en varias ventanas y realizar una media móvil, una suma acumulativa o una clasificación.\n",
    "\n",
    "En spark todas las agregaciones se realizan a través de funciones.\\\n",
    "Las funciones de agregación están diseñadas para realizar la agregación en un conjunto de filas, yasea que ese conjunto de filas consista en todas las filas o en un subgrupo de filas en un DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vuelos = spark.read.parquet('./data/flights/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 08:45:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|1    |1  |4          |AS     |98           |N407AS     |ANC           |SEA                |5                  |2354          |-11            |21      |15        |205           |194         |169     |1448    |404      |4      |430              |408         |-22          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |2336         |N3KUAA     |LAX           |PBI                |10                 |2             |-8             |12      |14        |280           |279         |263     |2330    |737      |4      |750              |741         |-9           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |US     |840          |N171US     |SFO           |CLT                |20                 |18            |-2             |16      |34        |286           |293         |266     |2296    |800      |11     |806              |811         |5            |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |258          |N3HYAA     |LAX           |MIA                |20                 |15            |-5             |15      |30        |285           |281         |258     |2342    |748      |8      |805              |756         |-9           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AS     |135          |N527AS     |SEA           |ANC                |25                 |24            |-1             |11      |35        |235           |215         |199     |1448    |254      |5      |320              |259         |-21          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Funciones count, countDistinct y aprox_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/flights/dataframe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- cantidad: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|nombre|color|cantidad|\n",
      "+------+-----+--------+\n",
      "|  Jose| azul|    1900|\n",
      "|  null| null|    1700|\n",
      "|  null| rojo|    1300|\n",
      "|  Juan| rojo|    1500|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **count** es una agregación que nos permite saber la cantidad de elementos de un grupo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|conteo_nombre|conteo_color|\n",
      "+-------------+------------+\n",
      "|            2|           3|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a contar la cantidad de nombres y de colores del DF. Cuenta todos aquellos valores diferentes de null que se encuentren en la columna.\n",
    "\n",
    "df.select(\n",
    "    count('nombre').alias('conteo_nombre'),\n",
    "    count('color').alias('conteo_color')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+\n",
      "|conteo_nombre|conteo_color|conteo_general|\n",
      "+-------------+------------+--------------+\n",
      "|            2|           3|             4|\n",
      "+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Si queremos que también incluya también los null, que cuente todas las filas, le pasamos '*' como parámetro.\n",
    "\n",
    "df.select(\n",
    "    count('nombre').alias('conteo_nombre'),\n",
    "    count('color').alias('conteo_color'),\n",
    "    count('*').alias('conteo_general')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **countDistinct** admite los nulos y cuenta los valores diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|colores_dif|\n",
      "+-----------+\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contamos los colores que podemos encontrar en la columna color\n",
    "\n",
    "df.select(\n",
    "    countDistinct('color').alias('colores_dif')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **approx_count_distinct** realiza un recuento aproximado de la cantidad total de valores. Hay que tener en cuenta que contar el número exacto de elementos únicos en cada grupo en un gran conjunto de datos es una operación costosa y que se requiere mucho tiempo, en algunos casos de uso es suficiente a veces tener un recuento único aproximado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------+\n",
      "|count(DISTINCT AIRLINE)|approx_count_distinct(AIRLINE)|\n",
      "+-----------------------+------------------------------+\n",
      "|                     14|                            13|\n",
      "+-----------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_vuelos.select(\n",
    "    countDistinct('AIRLINE'),\n",
    "    approx_count_distinct('AIRLINE')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Funciones min y max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|menor_timepo|mayor_tiempo|\n",
      "+------------+------------+\n",
      "|           7|         690|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a hallar el tiempo mínimo y máximo que estuvieron los aviones en el aire\n",
    "\n",
    "df_vuelos.select(\n",
    "    min('AIR_TIME').alias('menor_timepo'),\n",
    "    max('AIR_TIME').alias('mayor_tiempo')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|min(AIRLINE_DELAY)|max(AIRLINE_DELAY)|\n",
      "+------------------+------------------+\n",
      "|                 0|              1971|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a ver el tiempo mínimo y máximo de retraso en los vuelos\n",
    "\n",
    "df_vuelos.select(\n",
    "    min('AIRLINE_DELAY'),\n",
    "    max('AIRLINE_DELAY')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Funciones sum, sum_distinct y avg\n",
    "\n",
    "* **sum** calcula la suma de los valores de una columna numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   sum_dis|\n",
      "+----------+\n",
      "|4785357409|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos la distancia total recorrida en todos los vuelos\n",
    "\n",
    "df_vuelos.select(\n",
    "    sum('DISTANCE').alias('sum_dis')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **sum_distinc** suma sólo los valores distintos de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum_dis_dif|\n",
      "+-----------+\n",
      "|    1442300|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.select(\n",
    "    sum_distinct('DISTANCE').alias('sum_dis_dif')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **avg** calcula el valor promedio de una columna numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|     promedio_aire|       prom_manual|\n",
      "+------------------+------------------+\n",
      "|113.51162809012519|113.51162809012519|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos la media de tiempo en el aire\n",
    "\n",
    "df_vuelos.select(\n",
    "    avg('AIR_TIME').alias('promedio_aire'),\n",
    "    (sum('AIR_TIME') / count('AIR_TIME')).alias('prom_manual')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Agregación con agrupación\n",
    "\n",
    "Las agregaciones generalmente se realizan en conjuntos de datos que contienen una o más columnas categóricas que tienen una baja cardinalidad. Por ejemplo, algunos valores categóricos son el sexo, la edad, nombre de la ciudad o nombre del país.\n",
    "\n",
    "Las agregaciones se realizan a través de funciones similares a las estudiadas anteriormente. Sin embargo, en lugar de realizar la agregación en el DF, realizaremos la agregación en cada uno de los subgrupos dentro de un DF.\n",
    "\n",
    "Realizar la agregación con agrupación es un proceson de dos pasos:\n",
    "\n",
    "1. realizar la agrupación mediante la transformación groupBy(col1, col2, ...) de las columnas que deseemos agrupar y ahí es donde se especifica por qué columnas debemos agrupar las filas. A diferencia de otras transformaciones que devuelven un DF, la transformación groupBy devuelve una instancia de la clase relacional grouped dataset a la que luego puedes aplicar una o más funciones de agregación.\n",
    "\n",
    "2. aplicar las funciones de agregación deseadas.\n",
    "\n",
    "Debemos tener en cuenta que la clase del relacional grouped dataset, que es lo que nos devuelve el groupBy, proporciona un conjunto de funciones estándares de agregación que puede aplicar a cada subgrupo. Dentro de esta funciones tenemos el **avg**, el **count**, el **mean**, el **max**, el **min** o el **sum** entre otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|ORIGIN_AIRPORT| count|\n",
      "+--------------+------+\n",
      "|           ATL|346836|\n",
      "|           ORD|285884|\n",
      "|           DFW|239551|\n",
      "|           DEN|196055|\n",
      "|           LAX|194673|\n",
      "|           SFO|148008|\n",
      "|           PHX|146815|\n",
      "|           IAH|146622|\n",
      "|           LAS|133181|\n",
      "|           MSP|112117|\n",
      "|           MCO|110982|\n",
      "|           SEA|110899|\n",
      "|           DTW|108500|\n",
      "|           BOS|107847|\n",
      "|           EWR|101772|\n",
      "|           CLT|100324|\n",
      "|           LGA| 99605|\n",
      "|           SLC| 97210|\n",
      "|           JFK| 93811|\n",
      "|           BWI| 86079|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos cuántos aeropuertos de origen tenemos. Agruparemos por los aeropuertos de origen y contaremos cuántos vuelos salieron de cada uno de ellos\n",
    "\n",
    "(df_vuelos.groupBy('ORIGIN_AIRPORT')\n",
    "    .count()\n",
    "    .orderBy(desc('count'))\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-----+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|count|\n",
      "+--------------+-------------------+-----+\n",
      "|           SFO|                LAX|13744|\n",
      "|           LAX|                SFO|13457|\n",
      "|           JFK|                LAX|12016|\n",
      "|           LAX|                JFK|12015|\n",
      "|           LAS|                LAX| 9715|\n",
      "|           LGA|                ORD| 9639|\n",
      "|           LAX|                LAS| 9594|\n",
      "|           ORD|                LGA| 9575|\n",
      "|           SFO|                JFK| 8440|\n",
      "|           JFK|                SFO| 8437|\n",
      "|           OGG|                HNL| 8313|\n",
      "|           HNL|                OGG| 8282|\n",
      "|           LAX|                ORD| 8256|\n",
      "|           ATL|                LGA| 8234|\n",
      "|           LGA|                ATL| 8215|\n",
      "|           ATL|                MCO| 8202|\n",
      "|           MCO|                ATL| 8202|\n",
      "|           SFO|                LAS| 7995|\n",
      "|           ORD|                LAX| 7941|\n",
      "|           LAS|                SFO| 7870|\n",
      "+--------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a agrupar por el aeropuerto origen y el aeropuerto destino, para contar cuáles fueron los trayectos más frecuentados\n",
    "\n",
    "(df_vuelos.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\n",
    "    .count()\n",
    "    .orderBy(desc('count'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5. Varias agregaciones por grupo\n",
    "\n",
    "La clase del relacional grouped dataset proporciona una función llamada **agg** que toma una o más expresiones de columna, lo que significa que puede usar cualquiera de las funciones de agregación estudiadas en los puntos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---+---+\n",
      "|ORIGIN_AIRPORT|tiempo_aire|min|max|\n",
      "+--------------+-----------+---+---+\n",
      "|           ATL|     343506| 15|614|\n",
      "|           ORD|     276554| 13|571|\n",
      "|           DFW|     232647| 11|534|\n",
      "|           DEN|     193402| 12|493|\n",
      "|           LAX|     192003| 14|409|\n",
      "|           PHX|     145552| 19|444|\n",
      "|           SFO|     145491|  8|389|\n",
      "|           IAH|     144019| 15|524|\n",
      "|           LAS|     131937| 25|429|\n",
      "|           MSP|     111055| 14|537|\n",
      "|           SEA|     110178| 17|412|\n",
      "|           MCO|     109532| 25|395|\n",
      "|           DTW|     106992| 15|341|\n",
      "|           BOS|     104804| 16|432|\n",
      "|           CLT|      99052| 17|379|\n",
      "|           EWR|      98341| 21|683|\n",
      "|           SLC|      96505| 18|419|\n",
      "|           LGA|      94834| 19|311|\n",
      "|           JFK|      91663| 29|690|\n",
      "|           BWI|      84329| 19|398|\n",
      "+--------------+-----------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a agrupar por el aeropuerto origen y agregaremos el conteo del tiempo que estuvieron esos aviones en el aire y el min y max de ese tiempo\n",
    "\n",
    "df_vuelos.groupBy('ORIGIN_AIRPORT').agg(\n",
    "    count('AIR_TIME').alias('tiempo_aire'),\n",
    "    min('AIR_TIME').alias('min'),\n",
    "    max('AIR_TIME').alias('max')\n",
    ").orderBy(desc('tiempo_aire')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+\n",
      "|MONTH|conteo_de_retrasos|        prom_dist|\n",
      "+-----+------------------+-----------------+\n",
      "|    7|            514384|841.4772794487611|\n",
      "|    8|            503956|834.8244276603413|\n",
      "|    6|            492847|835.6302716626612|\n",
      "|    3|            492138|816.0553268611494|\n",
      "|    5|            489641|823.3230588760807|\n",
      "|   10|            482878|816.4436127652134|\n",
      "|    4|            479251|817.0060476016745|\n",
      "|   12|            469717|837.8018926194103|\n",
      "|   11|            462367|820.2482434846529|\n",
      "|    9|            462153|815.8487523282274|\n",
      "|    1|            457013|803.2612794913696|\n",
      "|    2|            407663| 800.785449834689|\n",
      "+-----+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a agrupar por el mes, agregamos el conteo de vuelos que llegaron con retraso y vemos el promedio de distancia de esos vuelos\n",
    "\n",
    "df_vuelos.groupBy('MONTH').agg(\n",
    "    count('ARRIVAL_DELAY').alias('conteo_de_retrasos'),\n",
    "    avg('DISTANCE').alias('prom_dist')\n",
    ").orderBy(desc('conteo_de_retrasos')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6. Agregación con pivote\n",
    "\n",
    "Permite agregar los resultados de las funciones por cada uno de los valores diferentes de una columna.\\\n",
    "Si le especificamos una lista de valores diferentes para la columna pivote, lo que va a hacer en realidad es acelerar el proceso de rotación de esta columna, el proceso de pivoteo. De lo contrario, spark va a dedicar un poco de tiempo y de esfuerzo a encontrar una lista de los valores distintos por sí solo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----------+\n",
      "|nombre|sexo|peso|graduacion|\n",
      "+------+----+----+----------+\n",
      "|  Jose|   M|  80|      2000|\n",
      "| Hilda|   F|  50|      2000|\n",
      "|  Juan|   M|  75|      2000|\n",
      "| Pedro|   M|  76|      2001|\n",
      "|Katia+|   F|  65|      2001|\n",
      "+------+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_estudiantes = spark.read.parquet('./data/estudiantes.parquet')\n",
    "\n",
    "df_estudiantes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+\n",
      "|graduacion|   F|   M|\n",
      "+----------+----+----+\n",
      "|      2001|65.0|76.0|\n",
      "|      2000|50.0|77.5|\n",
      "+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a ver el peso promedio por sexo en cada año de graduación utilizando pivot\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|graduacion|F_avg(peso)|F_min(peso)|F_max(peso)|M_avg(peso)|M_min(peso)|M_max(peso)|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|      2001|       65.0|         65|         65|       76.0|         76|         76|\n",
      "|      2000|       50.0|         50|         50|       77.5|         75|         80|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Además del promedio de peso, vamos a obtener el mínimo y el máximo\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+\n",
      "|graduacion|M_avg(peso)|M_min(peso)|M_max(peso)|\n",
      "+----------+-----------+-----------+-----------+\n",
      "|      2001|       76.0|         76|         76|\n",
      "|      2000|       77.5|         75|         80|\n",
      "+----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos tomar sólo algunos de los elementos presentes en la columna sobre la que pivotemos, que le pasaremos en forma de lista\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo', ['M']).agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+\n",
      "|graduacion|F_avg(peso)|F_min(peso)|F_max(peso)|\n",
      "+----------+-----------+-----------+-----------+\n",
      "|      2001|       65.0|         65|         65|\n",
      "|      2000|       50.0|         50|         50|\n",
      "+----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lo mismo que en la celda anterior, pero para el el sexo femenino\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo', ['F']).agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
