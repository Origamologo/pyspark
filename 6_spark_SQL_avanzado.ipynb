{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import count, countDistinct, approx_count_distinct\n",
    "from pyspark.sql.functions import min, max, col\n",
    "from pyspark.sql.functions import sum, sum_distinct, avg\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agregaciones\n",
    "\n",
    "La realización de análisis interesante sobre Big Data generalmente implica algún tipo de agregación para resumir los datos con el fin de extraer patrones, conocimientos o simplemente generar informes resumidos.\n",
    "\n",
    "Las agregaciones generalmente requieren de alguna forma de agrupación, ya sea en todo el conjunto de datos o en una o más columnas, y luego aplican funciones de agregación como sumar, contar o promediar a cada grupo.\n",
    "\n",
    "Spark proporciona muchas funciones de agregación de uso común, así como la capacidad de agregar los valores de una colección que luego se pueden analizar más a fondo.\n",
    "\n",
    "La agrupación de filas se puede realizar en diferentes niveles y spark admite los siguientes niveles:\n",
    "\n",
    "* Tratar un DF como un grupo.\n",
    "\n",
    "* Dividir un DF en varios grupos utilizando una o más columnas y realizar una o más agregaciones en cada uno de estos grupos.\n",
    "\n",
    "* Dividir un DF en varias ventanas y realizar una media móvil, una suma acumulativa o una clasificación.\n",
    "\n",
    "En spark todas las agregaciones se realizan a través de funciones.\\\n",
    "Las funciones de agregación están diseñadas para realizar la agregación en un conjunto de filas, yasea que ese conjunto de filas consista en todas las filas o en un subgrupo de filas en un DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vuelos = spark.read.parquet('./data/flights/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/25 08:45:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|1    |1  |4          |AS     |98           |N407AS     |ANC           |SEA                |5                  |2354          |-11            |21      |15        |205           |194         |169     |1448    |404      |4      |430              |408         |-22          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |2336         |N3KUAA     |LAX           |PBI                |10                 |2             |-8             |12      |14        |280           |279         |263     |2330    |737      |4      |750              |741         |-9           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |US     |840          |N171US     |SFO           |CLT                |20                 |18            |-2             |16      |34        |286           |293         |266     |2296    |800      |11     |806              |811         |5            |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |258          |N3HYAA     |LAX           |MIA                |20                 |15            |-5             |15      |30        |285           |281         |258     |2342    |748      |8      |805              |756         |-9           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AS     |135          |N527AS     |SEA           |ANC                |25                 |24            |-1             |11      |35        |235           |215         |199     |1448    |254      |5      |320              |259         |-21          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Funciones count, countDistinct y aprox_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/flights/dataframe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- cantidad: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|nombre|color|cantidad|\n",
      "+------+-----+--------+\n",
      "|  Jose| azul|    1900|\n",
      "|  null| null|    1700|\n",
      "|  null| rojo|    1300|\n",
      "|  Juan| rojo|    1500|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **count** es una agregación que nos permite saber la cantidad de elementos de un grupo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|conteo_nombre|conteo_color|\n",
      "+-------------+------------+\n",
      "|            2|           3|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a contar la cantidad de nombres y de colores del DF. Cuenta todos aquellos valores diferentes de null que se encuentren en la columna.\n",
    "\n",
    "df.select(\n",
    "    count('nombre').alias('conteo_nombre'),\n",
    "    count('color').alias('conteo_color')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+\n",
      "|conteo_nombre|conteo_color|conteo_general|\n",
      "+-------------+------------+--------------+\n",
      "|            2|           3|             4|\n",
      "+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Si queremos que también incluya también los null, que cuente todas las filas, le pasamos '*' como parámetro.\n",
    "\n",
    "df.select(\n",
    "    count('nombre').alias('conteo_nombre'),\n",
    "    count('color').alias('conteo_color'),\n",
    "    count('*').alias('conteo_general')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **countDistinct** admite los nulos y cuenta los valores diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|colores_dif|\n",
      "+-----------+\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contamos los colores que podemos encontrar en la columna color\n",
    "\n",
    "df.select(\n",
    "    countDistinct('color').alias('colores_dif')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **approx_count_distinct** realiza un recuento aproximado de la cantidad total de valores. Hay que tener en cuenta que contar el número exacto de elementos únicos en cada grupo en un gran conjunto de datos es una operación costosa y que se requiere mucho tiempo, en algunos casos de uso es suficiente a veces tener un recuento único aproximado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------+\n",
      "|count(DISTINCT AIRLINE)|approx_count_distinct(AIRLINE)|\n",
      "+-----------------------+------------------------------+\n",
      "|                     14|                            13|\n",
      "+-----------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_vuelos.select(\n",
    "    countDistinct('AIRLINE'),\n",
    "    approx_count_distinct('AIRLINE')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Funciones min y max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|menor_timepo|mayor_tiempo|\n",
      "+------------+------------+\n",
      "|           7|         690|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a hallar el tiempo mínimo y máximo que estuvieron los aviones en el aire\n",
    "\n",
    "df_vuelos.select(\n",
    "    min('AIR_TIME').alias('menor_timepo'),\n",
    "    max('AIR_TIME').alias('mayor_tiempo')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|min(AIRLINE_DELAY)|max(AIRLINE_DELAY)|\n",
      "+------------------+------------------+\n",
      "|                 0|              1971|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a ver el tiempo mínimo y máximo de retraso en los vuelos\n",
    "\n",
    "df_vuelos.select(\n",
    "    min('AIRLINE_DELAY'),\n",
    "    max('AIRLINE_DELAY')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Funciones sum, sum_distinct y avg\n",
    "\n",
    "* **sum** calcula la suma de los valores de una columna numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   sum_dis|\n",
      "+----------+\n",
      "|4785357409|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos la distancia total recorrida en todos los vuelos\n",
    "\n",
    "df_vuelos.select(\n",
    "    sum('DISTANCE').alias('sum_dis')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **sum_distinc** suma sólo los valores distintos de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum_dis_dif|\n",
      "+-----------+\n",
      "|    1442300|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.select(\n",
    "    sum_distinct('DISTANCE').alias('sum_dis_dif')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **avg** calcula el valor promedio de una columna numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|     promedio_aire|       prom_manual|\n",
      "+------------------+------------------+\n",
      "|113.51162809012519|113.51162809012519|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos la media de tiempo en el aire\n",
    "\n",
    "df_vuelos.select(\n",
    "    avg('AIR_TIME').alias('promedio_aire'),\n",
    "    (sum('AIR_TIME') / count('AIR_TIME')).alias('prom_manual')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Agregación con agrupación\n",
    "\n",
    "Las agregaciones generalmente se realizan en conjuntos de datos que contienen una o más columnas categóricas que tienen una baja cardinalidad. Por ejemplo, algunos valores categóricos son el sexo, la edad, nombre de la ciudad o nombre del país.\n",
    "\n",
    "Las agregaciones se realizan a través de funciones similares a las estudiadas anteriormente. Sin embargo, en lugar de realizar la agregación en el DF, realizaremos la agregación en cada uno de los subgrupos dentro de un DF.\n",
    "\n",
    "Realizar la agregación con agrupación es un proceson de dos pasos:\n",
    "\n",
    "1. realizar la agrupación mediante la transformación groupBy(col1, col2, ...) de las columnas que deseemos agrupar y ahí es donde se especifica por qué columnas debemos agrupar las filas. A diferencia de otras transformaciones que devuelven un DF, la transformación groupBy devuelve una instancia de la clase relacional grouped dataset a la que luego puedes aplicar una o más funciones de agregación.\n",
    "\n",
    "2. aplicar las funciones de agregación deseadas.\n",
    "\n",
    "Debemos tener en cuenta que la clase del relacional grouped dataset, que es lo que nos devuelve el groupBy, proporciona un conjunto de funciones estándares de agregación que puede aplicar a cada subgrupo. Dentro de esta funciones tenemos el **avg**, el **count**, el **mean**, el **max**, el **min** o el **sum** entre otras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|ORIGIN_AIRPORT| count|\n",
      "+--------------+------+\n",
      "|           ATL|346836|\n",
      "|           ORD|285884|\n",
      "|           DFW|239551|\n",
      "|           DEN|196055|\n",
      "|           LAX|194673|\n",
      "|           SFO|148008|\n",
      "|           PHX|146815|\n",
      "|           IAH|146622|\n",
      "|           LAS|133181|\n",
      "|           MSP|112117|\n",
      "|           MCO|110982|\n",
      "|           SEA|110899|\n",
      "|           DTW|108500|\n",
      "|           BOS|107847|\n",
      "|           EWR|101772|\n",
      "|           CLT|100324|\n",
      "|           LGA| 99605|\n",
      "|           SLC| 97210|\n",
      "|           JFK| 93811|\n",
      "|           BWI| 86079|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Veamos cuántos aeropuertos de origen tenemos. Agruparemos por los aeropuertos de origen y contaremos cuántos vuelos salieron de cada uno de ellos\n",
    "\n",
    "(df_vuelos.groupBy('ORIGIN_AIRPORT')\n",
    "    .count()\n",
    "    .orderBy(desc('count'))\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-----+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|count|\n",
      "+--------------+-------------------+-----+\n",
      "|           SFO|                LAX|13744|\n",
      "|           LAX|                SFO|13457|\n",
      "|           JFK|                LAX|12016|\n",
      "|           LAX|                JFK|12015|\n",
      "|           LAS|                LAX| 9715|\n",
      "|           LGA|                ORD| 9639|\n",
      "|           LAX|                LAS| 9594|\n",
      "|           ORD|                LGA| 9575|\n",
      "|           SFO|                JFK| 8440|\n",
      "|           JFK|                SFO| 8437|\n",
      "|           OGG|                HNL| 8313|\n",
      "|           HNL|                OGG| 8282|\n",
      "|           LAX|                ORD| 8256|\n",
      "|           ATL|                LGA| 8234|\n",
      "|           LGA|                ATL| 8215|\n",
      "|           ATL|                MCO| 8202|\n",
      "|           MCO|                ATL| 8202|\n",
      "|           SFO|                LAS| 7995|\n",
      "|           ORD|                LAX| 7941|\n",
      "|           LAS|                SFO| 7870|\n",
      "+--------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a agrupar por el aeropuerto origen y el aeropuerto destino, para contar cuáles fueron los trayectos más frecuentados\n",
    "\n",
    "(df_vuelos.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\n",
    "    .count()\n",
    "    .orderBy(desc('count'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-5. Varias agregaciones por grupo\n",
    "\n",
    "La clase del relacional grouped dataset proporciona una función llamada **agg** que toma una o más expresiones de columna, lo que significa que puede usar cualquiera de las funciones de agregación estudiadas en los puntos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---+---+\n",
      "|ORIGIN_AIRPORT|tiempo_aire|min|max|\n",
      "+--------------+-----------+---+---+\n",
      "|           ATL|     343506| 15|614|\n",
      "|           ORD|     276554| 13|571|\n",
      "|           DFW|     232647| 11|534|\n",
      "|           DEN|     193402| 12|493|\n",
      "|           LAX|     192003| 14|409|\n",
      "|           PHX|     145552| 19|444|\n",
      "|           SFO|     145491|  8|389|\n",
      "|           IAH|     144019| 15|524|\n",
      "|           LAS|     131937| 25|429|\n",
      "|           MSP|     111055| 14|537|\n",
      "|           SEA|     110178| 17|412|\n",
      "|           MCO|     109532| 25|395|\n",
      "|           DTW|     106992| 15|341|\n",
      "|           BOS|     104804| 16|432|\n",
      "|           CLT|      99052| 17|379|\n",
      "|           EWR|      98341| 21|683|\n",
      "|           SLC|      96505| 18|419|\n",
      "|           LGA|      94834| 19|311|\n",
      "|           JFK|      91663| 29|690|\n",
      "|           BWI|      84329| 19|398|\n",
      "+--------------+-----------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a agrupar por el aeropuerto origen y agregaremos el conteo del tiempo que estuvieron esos aviones en el aire y el min y max de ese tiempo\n",
    "\n",
    "df_vuelos.groupBy('ORIGIN_AIRPORT').agg(\n",
    "    count('AIR_TIME').alias('tiempo_aire'),\n",
    "    min('AIR_TIME').alias('min'),\n",
    "    max('AIR_TIME').alias('max')\n",
    ").orderBy(desc('tiempo_aire')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+\n",
      "|MONTH|conteo_de_retrasos|        prom_dist|\n",
      "+-----+------------------+-----------------+\n",
      "|    7|            514384|841.4772794487611|\n",
      "|    8|            503956|834.8244276603413|\n",
      "|    6|            492847|835.6302716626612|\n",
      "|    3|            492138|816.0553268611494|\n",
      "|    5|            489641|823.3230588760807|\n",
      "|   10|            482878|816.4436127652134|\n",
      "|    4|            479251|817.0060476016745|\n",
      "|   12|            469717|837.8018926194103|\n",
      "|   11|            462367|820.2482434846529|\n",
      "|    9|            462153|815.8487523282274|\n",
      "|    1|            457013|803.2612794913696|\n",
      "|    2|            407663| 800.785449834689|\n",
      "+-----+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a agrupar por el mes, agregamos el conteo de vuelos que llegaron con retraso y vemos el promedio de distancia de esos vuelos\n",
    "\n",
    "df_vuelos.groupBy('MONTH').agg(\n",
    "    count('ARRIVAL_DELAY').alias('conteo_de_retrasos'),\n",
    "    avg('DISTANCE').alias('prom_dist')\n",
    ").orderBy(desc('conteo_de_retrasos')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-6. Agregación con pivote\n",
    "\n",
    "Permite agregar los resultados de las funciones por cada uno de los valores diferentes de una columna.\\\n",
    "Si le especificamos una lista de valores diferentes para la columna pivote, lo que va a hacer en realidad es acelerar el proceso de rotación de esta columna, el proceso de pivoteo. De lo contrario, spark va a dedicar un poco de tiempo y de esfuerzo a encontrar una lista de los valores distintos por sí solo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----------+\n",
      "|nombre|sexo|peso|graduacion|\n",
      "+------+----+----+----------+\n",
      "|  Jose|   M|  80|      2000|\n",
      "| Hilda|   F|  50|      2000|\n",
      "|  Juan|   M|  75|      2000|\n",
      "| Pedro|   M|  76|      2001|\n",
      "|Katia+|   F|  65|      2001|\n",
      "+------+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_estudiantes = spark.read.parquet('./data/estudiantes.parquet')\n",
    "\n",
    "df_estudiantes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+\n",
      "|graduacion|   F|   M|\n",
      "+----------+----+----+\n",
      "|      2001|65.0|76.0|\n",
      "|      2000|50.0|77.5|\n",
      "+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a ver el peso promedio por sexo en cada año de graduación utilizando pivot\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|graduacion|F_avg(peso)|F_min(peso)|F_max(peso)|M_avg(peso)|M_min(peso)|M_max(peso)|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|      2001|       65.0|         65|         65|       76.0|         76|         76|\n",
      "|      2000|       50.0|         50|         50|       77.5|         75|         80|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Además del promedio de peso, vamos a obtener el mínimo y el máximo\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+\n",
      "|graduacion|M_avg(peso)|M_min(peso)|M_max(peso)|\n",
      "+----------+-----------+-----------+-----------+\n",
      "|      2001|       76.0|         76|         76|\n",
      "|      2000|       77.5|         75|         80|\n",
      "+----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos tomar sólo algunos de los elementos presentes en la columna sobre la que pivotemos, que le pasaremos en forma de lista\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo', ['M']).agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+\n",
      "|graduacion|F_avg(peso)|F_min(peso)|F_max(peso)|\n",
      "+----------+-----------+-----------+-----------+\n",
      "|      2001|       65.0|         65|         65|\n",
      "|      2000|       50.0|         50|         50|\n",
      "+----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lo mismo que en la celda anterior, pero para el el sexo femenino\n",
    "\n",
    "df_estudiantes.groupBy('graduacion').pivot('sexo', ['F']).agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Joins\n",
    "\n",
    "Al realizar un join se combinarán las columnas de dos conjuntos de datos. Estos podrían ser diferentes o iguales y el DF combinado contendrá columnas de ambos lados.\n",
    "\n",
    "Realizar un join de dos conjuntos de datos requiere que especificamos dos piezas de información:\n",
    "\n",
    "1. Una expresión de **join** que especifica qué columnas de cada conjunto de datos deben usarse para determinar qué filas de ambos conjuntos de datos se incluirán en el conjunto de datos combinados.\n",
    "\n",
    "2. El tipo de join, que determina qué se debe incluir en el conjunto de datos combinados.\n",
    "\n",
    "Tipos de join admitidos en sparkSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empleados = spark.read.parquet('./data/empleados')\n",
    "\n",
    "df_departamentos = spark.read.parquet('./data/departamentos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "|  Luis|      33|\n",
      "| Katia|      33|\n",
      "|  Raul|      34|\n",
      "| Pedro|       0|\n",
      "| Laura|      34|\n",
      "|Sandro|      31|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|nombre_dpto|\n",
      "+---+-----------+\n",
      "| 31|     letras|\n",
      "| 33|    derecho|\n",
      "| 34| matemática|\n",
      "| 35|informática|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_departamentos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Inner Join\n",
    "\n",
    "Devuelve filas de ambos conjuntos de datos cuando la expresión de join se evalúa como verdadera. Las filas que no tengan valores de columna coincidente se excluirán del conjunto de datos resultantes. En sparkSQL es el tipo de join predeterminado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Al ser el join predeterminado, no es necesario especificarlo\n",
    "\n",
    "innerjoin_df = df_empleados.join(df_departamentos, col('num_dpto') == col('id'))\n",
    "\n",
    "innerjoin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos hacer lo mismo que en la celda anterior, pero especificándole el tipo de join, aunque en este caso no sea necesario\n",
    "\n",
    "innerjoin_df2 = df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'inner')\n",
    "\n",
    "innerjoin_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Y podemos hacer exactamente lo mismo, pero especificándole la condición con un where\n",
    "\n",
    "innerjoin_df3 = df_empleados.join(df_departamentos).where(col('num_dpto') == col('id'))\n",
    "\n",
    "innerjoin_df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Left Outer Join\n",
    "\n",
    "Devuelve filas del conjunto de datos de la izquierda, incluso cuando la expresión de join se evalúa como falsa. Rellenará los datos faltantes con nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Pedro|       0|null|       null|\n",
      "| Laura|      34|  34| matemática|\n",
      "|Sandro|      31|  31|     letras|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'leftouter').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Pedro|       0|null|       null|\n",
      "| Laura|      34|  34| matemática|\n",
      "|Sandro|      31|  31|     letras|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# También podemos especificar que es leftouter de este modo:\n",
    "\n",
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'left_outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Pedro|       0|null|       null|\n",
      "| Laura|      34|  34| matemática|\n",
      "|Sandro|      31|  31|     letras|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Y aún tenemos este modo de indicar que es left outer\n",
    "\n",
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Right Outer Join\n",
    "\n",
    "Devuelve filas del conjunto de datos de la derecha, incluso cuando la expresión de join se evalúa como falsa. Rellenará los datos faltantes con nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|Sandro|      31| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  null|    null| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'rightouter').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|Sandro|      31| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  null|    null| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# También podemos especificar que es rightouter de este modo:\n",
    "\n",
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'right_outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|Sandro|      31| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  null|    null| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Y aún tenemos este modo de indicar que es right outer\n",
    "\n",
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'right').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Full Outer Join\n",
    "\n",
    "Devuelve filas de ambos conjuntos de datos, incluso cuando la expresión de join se evalúa como falsa. Su comportamiento es el mismo que el de combinar leftouter y rightouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "| Pedro|       0|null|       null|\n",
      "|Sandro|      31|  31|     letras|\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Laura|      34|  34| matemática|\n",
      "|  null|    null|  35|informática|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. Left Anti Join\n",
    "\n",
    "Devuelve filas solo del conjunto de datos de la izquierda cuando la expresión de join se evalúa como falsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "| Pedro|       0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|nombre_dpto|\n",
      "+---+-----------+\n",
      "| 35|informática|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_departamentos.join(df_empleados, col('num_dpto') == col('id'), 'left_anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-6. Left Semi Join\n",
    "\n",
    "Devuelve filas del conjunto de datos de la izquierda cuando la expresión de join se evalúa como verdadera y no incluye las columnas del conjunto de datos de la derecha. Las filas que no tengan valores de columna coincidente se excluirán del conjunto de datos resultantes.\\\n",
    "Su comportamiento es el opuesto al left anti join, ya que aquí el conjunto de los datos resultante contiene sólo las filas coincidentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "|  Luis|      33|\n",
      "| Katia|      33|\n",
      "|  Raul|      34|\n",
      "| Laura|      34|\n",
      "|Sandro|      31|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.join(df_departamentos, col('num_dpto') == col('id'), 'left_semi').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-7. Cross Join\n",
    "\n",
    "Devuelve filas combinando cada fila del conjunto de datos de la izquierda con cada fila del conjunto de datos de la derecha. El número de filas será un producto del tamaño de cada conjunto de datos. Por esta razón, la forma de utilizar este tipo de join es mediante el uso explícito de una transformación dedicada en DF, en lugar de especificar este tipo de combinación como una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 31|     letras|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "|  Luis|      33| 34| matemática|\n",
      "|  Luis|      33| 35|informática|\n",
      "| Katia|      33| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cross = df_empleados.crossJoin(df_departamentos)\n",
    "\n",
    "df_cross.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cross.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manejo de nombres de columnas duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear un nuevo DF a partir de df_departamentos con una nueva columna idéntica a 'id'\n",
    "\n",
    "df_departamentos_dup = df_departamentos.withColumn('num_dpto', col('id'))\n",
    "\n",
    "df_departamentos_dup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_empleados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto devuelve un error de ambigüedad porque hay dos columnas con el mismo nombre y, aunque pertenezcan a distitos DFs, no sabe a cuál recurrir\n",
    "\n",
    "df_empleados.join(df_departamentos_dup, col('num_dpto') == col('num_dpto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para el código de la celda anterior funcione, debemos informarle de qué DF queremos que tome cada columna\n",
    "\n",
    "df_con_duplicados = df_empleados.join(df_departamentos_dup, df_empleados['num_dpto'] == df_departamentos_dup['num_dpto'])\n",
    "\n",
    "df_con_duplicados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto también nos va a arrojar un error de ambigüedad\n",
    "\n",
    "df_con_duplicados.select(['num_dpto']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">El nuevo DF mantiene la informacíon de qué DF aportó cada columna, de modo que aunque dos tengan el mismo nombre. Para eliminar la ambigüedad se le puede pedir a spark que a la columna le ponga como prefijo el nombre del DF del que proviene.\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|num_dpto|\n",
      "+--------+\n",
      "|      33|\n",
      "|      33|\n",
      "|      34|\n",
      "|      34|\n",
      "|      31|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_con_duplicados.select(df_empleados['num_dpto']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Otra alternativa para evitar errores de ambigüedad sería renombrar las columnas antes de hacer el join\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">La alternativa más recomendada es usar una columna de unión para hacer el join\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_dpto: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aquí le decimos la columna por la cuál tiene que hacer el join, spark sobreentiende uqe son iguales y va a resolver la columna número de departamentos como una sola columna, sin duplicarla en el esquema\n",
    "\n",
    "df_con_duplicados2 = df_empleados.join(df_departamentos_dup, 'num_dpto')\n",
    "\n",
    "df_con_duplicados2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_dpto: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Le podemos dar la misma información que en la celda anterior para que ni caiga en error de ambigüedad ni duplique columnas, \n",
    "# mediante una lista. Esta opción es muy importante cuando queremos unir por más de una columna.\n",
    "\n",
    "df_empleados.join(df_departamentos_dup, ['num_dpto']).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Shuffle Hash Join y Broadcast Hash Join\n",
    "\n",
    "Son las dos estrategias que emplea spark para realizar los join.\n",
    "\n",
    "Conceptualmente, la unión de datos consiste en combinar las columnas de las filas de dos conjuntos de datos que cumplen la condición especificada en la expresión de unión. Para hacer eso, esas filas con los mismos valores de columna deben ubicarse en la misma partición.\n",
    "\n",
    "Join es una de las operaciones más costosas de Spark porque requiere mover una gran cantidad de datos de varias máquinas a través de una red. Al mover datos a través de una red, los datos generalmente pasan por un proceso de serialización y de serialización de datos. Es importante reducir la frecuencia con que unimos grandes conjuntos de datos siempre que sea posible.\n",
    "\n",
    "A un alto nivel existen dos estrategias diferentes que Spark usa para unir dos conjuntos de datos:\n",
    "\n",
    "* **Shuffle Hash Join** consta de dos pasos:\n",
    "\n",
    "    1. Calcular el valor hash de las columnas en la expresión de join de cada fila en cada conjunto de datos y luego mover esas filas con el mismo valor hash a la misma partición. Para determinar a qué partición se moverá una fila en particular Spark realiza una operación aritmética simple que calcula el módulo del valor hash por el número de particiones.\n",
    "\n",
    "    2. Combina las columnas de aquellas filas que tienen el mismo valor hash de columna.\n",
    "\n",
    "* **Broadcast Hash Join** evita el movimiento de ambos conjuntos de datos y en su lugar sólo mueve el más pequeño. Ese DF tiene que ser lo suficientemente pequeño como para que pueda ser enviado a todos los executors. Consta de dos pasos:\n",
    "\n",
    "    1. Transmitir una copia de todo el conjunto de datos más pequeños a cada una de las particiones del conjunto de datos más grandes.\n",
    "\n",
    "    2. Recorrer cada fila en el conjunto de datos más grande y buscar las filas correspondientes en el conjunto de datos más pequeño con valores de columna coincidentes.\n",
    "\n",
    "El criterio principal para seleccionar una estrategia en particular se basa en el tamaño de los dos conjuntos de datos. Cuando el tamaño de ambos conjuntos de datos es grande, se utiliza **shuffle hash join**.\\\n",
    "Cuando el tamaño de uno de los conjuntos de datos es lo suficientemente pequeño como para caber en la memoria de los ejecutores se utiliza **broadcast hash join**.\n",
    "\n",
    ">\\\n",
    ">Es preferible usar **broadcast hash join** cuando sea posible.\n",
    ">\n",
    "><br>\n",
    "\n",
    "SparkSQL suele ser capaz de determinar automáticamente si se debe usar **broadcast hash join** o **shuffle hash join** en función de algunas estadísticas que tiene sobre los conjuntos de datos mientras los lee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a hacer un join indicándole a qué DF le va a hacer el broadcast con la función broadcast(<nombre_DF>) \n",
    "\n",
    "df_empleados.join(broadcast(df_departamentos), col('num_dpto') == col('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [num_dpto#4443L], [id#4446L], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(num_dpto#4443L)\n",
      "   :  +- FileScan parquet [nombre#4442,num_dpto#4443L] Batched: true, DataFilters: [isnotnull(num_dpto#4443L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/bluetab/Documents/scala/pyspark/data/empleados], PartitionFilters: [], PushedFilters: [IsNotNull(num_dpto)], ReadSchema: struct<nombre:string,num_dpto:bigint>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=3142]\n",
      "      +- Filter isnotnull(id#4446L)\n",
      "         +- FileScan parquet [id#4446L,nombre_dpto#4447] Batched: true, DataFilters: [isnotnull(id#4446L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/bluetab/Documents/scala/pyspark/data/departamentos], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,nombre_dpto:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Con la función explain() podemos ver el plan de ejecución con el cual Spark va a llevar a cabo esta consulta que le acabamos de lanzar.\n",
    "\n",
    "df_empleados.join(broadcast(df_departamentos), col('num_dpto') == col('id')).explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
