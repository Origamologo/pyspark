{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import datediff, months_between, last_day\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second\n",
    "from pyspark.sql.functions import ltrim, rtrim, trim\n",
    "from pyspark.sql.functions import lpad, rpad\n",
    "from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import size, sort_array, array_contains\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, LongType\n",
    "from pyspark.sql.functions import from_json, to_json\n",
    "\n",
    "from pyspark.sql.functions import when, lit, coalesce\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number, rank, dense_rank\n",
    "from pyspark.sql.functions import min, max, avg\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Funciones de fecha y hora\n",
    "\n",
    "### 1-1. Convertir de string a fecha y darle formato\n",
    "to_date, to_timestamp, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_convertir = spark.read.parquet('./data/convertir/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_convertir.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------------+\n",
      "|date      |timestamp              |date_str  |ts_str          |\n",
      "+----------+-----------------------+----------+----------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|01-01-2021|18-08-2021 46:58|\n",
      "+----------+-----------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_convertir.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a convertir los dato a formato date y timestamp\n",
    "\n",
    "df_convertir1 = df_convertir.select(\n",
    "    to_date(col('date')).alias('date1'),\n",
    "    to_timestamp(col('timestamp')).alias('ts1'),\n",
    "    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n",
    "    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-------------------+\n",
      "|date1     |ts1                    |date2     |ts2                |\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|2021-01-01|2021-08-18 00:46:58|\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_convertir1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date1: date (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_convertir1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_format(date1, dd-MM-yyyy)|\n",
      "+------------------------------+\n",
      "|                    01-01-2021|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos cambiar el formato de una fecha\n",
    "\n",
    "df_convertir1.select(\n",
    "    date_format(col('date1'), 'dd-MM-yyyy')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Cálculos de fecha y hora\n",
    "datediff, months_between, last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calculo = spark.read.parquet('./data/calculo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-------------------+\n",
      "|nombre|fecha_ingreso|fecha_salida|       baja_sistema|\n",
      "+------+-------------+------------+-------------------+\n",
      "|  Jose|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|\n",
      "|Mayara|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|\n",
      "+------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_calculo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+--------------+\n",
      "|nombre|dias|      meses|ultimo_dia_mes|\n",
      "+------+----+-----------+--------------+\n",
      "|  Jose| 317|10.41935484|    2021-11-30|\n",
      "|Mayara| 292| 9.61290323|    2021-11-30|\n",
      "+------+----+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a calcular la diferencia entre la fecha de salida y la fecha de ingreso\n",
    "\n",
    "df_calculo.select(\n",
    "    col('nombre'),\n",
    "    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n",
    "    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n",
    "    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-----------+\n",
      "|nombre|fecha_ingreso|mas_14_dias|menos_1_dia|\n",
      "+------+-------------+-----------+-----------+\n",
      "|  Jose|   2021-01-01| 2021-01-15| 2020-12-31|\n",
      "|Mayara|   2021-02-06| 2021-02-20| 2021-02-05|\n",
      "+------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a sumar y restar fechas, le sumaremos a fecha_ingreso 14 días y le restaremos 1\n",
    "\n",
    "df_calculo.select(\n",
    "    col('nombre'),\n",
    "    col('fecha_ingreso'),\n",
    "    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n",
    "    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Extraer valores específicos de una columna date\n",
    "year, month, dayofmonth, dayofyear, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|       baja_sistema|year(baja_sistema)|month(baja_sistema)|dayofmonth(baja_sistema)|dayofyear(baja_sistema)|hour(baja_sistema)|minute(baja_sistema)|second(baja_sistema)|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|2021-10-14 15:35:59|              2021|                 10|                      14|                    287|                15|                  35|                  59|\n",
      "|2021-11-25 10:35:55|              2021|                 11|                      25|                    329|                10|                  35|                  55|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_calculo.select(\n",
    "    col('baja_sistema'),\n",
    "    year(col('baja_sistema')),\n",
    "    month(col('baja_sistema')),\n",
    "    dayofmonth(col('baja_sistema')),\n",
    "    dayofyear(col('baja_sistema')),\n",
    "    hour(col('baja_sistema')),\n",
    "    minute(col('baja_sistema')),\n",
    "    second(col('baja_sistema'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones para trabajar con strings\n",
    "\n",
    "### 2-1. Transformación de un string\n",
    "\n",
    "Para eliminar espacios sobrantes:\n",
    "\n",
    "* ltrim, rtrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_strings = spark.read.parquet('./data/strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "| Spark |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_strings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "| ltrim| rtrim| trim|\n",
      "+------+------+-----+\n",
      "|Spark | Spark|Spark|\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a eliminar los espacios sobrantes en la palabra Spark\n",
    "\n",
    "df_strings.select(\n",
    "    ltrim('nombre').alias('ltrim'), # Elimina espacios a la izquierda\n",
    "    rtrim('nombre').alias('rtrim'), # Elimina espacios a la derecha\n",
    "    trim('nombre').alias('trim') # Elimina espacios tanto a la izquierda como a la derecha\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rellenar un string con un caracter fijo:\n",
    "\n",
    "* lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    lpad|    rpad|\n",
      "+--------+--------+\n",
      "|---Spark|Spark===|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_strings.select(\n",
    "    trim(col('nombre')).alias('trim') # Primero eliminamos los espacios en blanco\n",
    ").select(\n",
    "    lpad(col('trim'), 8, '-').alias('lpad'), # Le decimos el número total de caracteres que queremos y el caracter que queremos añadir a la izquierda\n",
    "    rpad(col('trim'), 8, '=').alias('rpad') # Le decimos el número total de caracteres que queremos y el caracter que queremos añadir a la derecha\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para concatenar, cambiar entre mayúsculas y minúsculas y revertir un string:\n",
    "\n",
    "* concat_ws, lower, upper, initcap, reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|sujeto|verbo|   adjetivo|\n",
      "+------+-----+-----------+\n",
      "| Spark|   es|maravilloso|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_strings1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])\n",
    "\n",
    "df_strings1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               frase|           minuscula|           mayuscula|             initcap|             reversa|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Spark es maravilloso|spark es maravilloso|SPARK ES MARAVILLOSO|Spark Es Maravilloso|osollivaram se krapS|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_strings1.select(\n",
    "    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase') # Concatenamos las tres columnas con un espacio\n",
    ").select(\n",
    "    col('frase'), # Al resultado de la instrucción anterior...\n",
    "    lower(col('frase')).alias('minuscula'), # ...lo pasamos todo a minúsculas\n",
    "    upper(col('frase')).alias('mayuscula'), # ...lo pasamos todo a mayúsculas\n",
    "    initcap(col('frase')).alias('initcap'), # ...ponemos en mayúscula la inicial de cada palabra\n",
    "    reverse(col('frase')).alias('reversa') # ...revertimos el orden de los caracteres del string\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Aplicación de expresiones regulares\n",
    "\n",
    "Nos permite reemplazar alguna parte de un string o extraer ciertas partes de un string según un patrón:\n",
    "\n",
    "* regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|frase                     |\n",
      "+--------------------------+\n",
      "| voy a casa por mis llaves|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_strings2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])\n",
    "\n",
    "df_strings2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|nueva_frase             |\n",
      "+------------------------+\n",
      "| ir a casa ir mis llaves|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reemplazamos un patrón por lo que nosotros queramos\n",
    "\n",
    "df_strings2.select(\n",
    "    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase') # Sustituímos las palabras 'voy' y 'por' por 'ir'\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funciones para trabajar con colecciones\n",
    "\n",
    "Están diseñadas para trabajar con tipos de datos complejos como array, map y estructuras.\n",
    "\n",
    "### 3-1. Trabajar con arrays:\n",
    "\n",
    "* size, sort_array, array_contains, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas                                      |\n",
      "+-----+--------------------------------------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_colecc_parquet = spark.read.parquet('./data/colecciones/parquet/')\n",
    "df_colecc_parquet.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dia: string (nullable = true)\n",
      " |-- tareas: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_colecc_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------+-----------+\n",
      "|tamaño|arreglo_ordenado                            |buscar_agua|\n",
      "+------+--------------------------------------------+-----------+\n",
      "|3     |[buscar agua, hacer la tarea, lavar el auto]|true       |\n",
      "+------+--------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_colecc_parquet.select(\n",
    "    size(col('tareas')).alias('tamaño'), # Nos da el tamaño de la colección\n",
    "    sort_array(col('tareas')).alias('arreglo_ordenado'), # Nos ordena la collection ya sea por orden alfabético o numérico\n",
    "    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua') # Nos permite buscar valores específicos dentro de la collection\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|  dia|        tareas|\n",
      "+-----+--------------+\n",
      "|lunes|hacer la tarea|\n",
      "|lunes|   buscar agua|\n",
      "|lunes| lavar el auto|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_colecc_parquet.select(\n",
    "    col('dia'),\n",
    "    explode(col('tareas')).alias('tareas') # Transforma cada elemento de la collection en una fila del DF\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Trabajar con json\n",
    "\n",
    "Para convertir una cadena JSON en un tipo de datos de estructura de Spark, necesitamos describir su estructura, valga la redundancia.\\\n",
    "Para ello vamos a necesitar crear un esquema JSON que proporcionaremos luego a la función from_json para que determine cuál es la estructura de nuestra columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|tareas_str                                                                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "|{\"dia\": \"lunes\",\"tareas\": [\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str = spark.read.parquet('./data/colecciones/JSON')\n",
    "json_df_str.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tareas_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el esquema JSON con dos columnas cuyos nombres coinciden con las keys de la estructura JSON\n",
    "\n",
    "schema_json = StructType(\n",
    "    [\n",
    "     StructField('dia', StringType(), True),\n",
    "     StructField('tareas', ArrayType(StringType()), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A from_json le pasamos la columna que contiene el string en forma de json y el esquema que va a necesitar para discernir \n",
    "# los tipos de datos que tiene dentro esa columna.\n",
    "\n",
    "json_df = json_df_str.select(\n",
    "    from_json(col('tareas_str'), schema_json).alias('por_hacer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- por_hacer: struct (nullable = true)\n",
      " |    |-- dia: string (nullable = true)\n",
      " |    |-- tareas: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------------+--------------+\n",
      "|por_hacer.dia|por_hacer.tareas                            |primer_tarea  |\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "|lunes        |[hacer la tarea, buscar agua, lavar el auto]|hacer la tarea|\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a extraer información del json_df que hemos creado\n",
    "\n",
    "json_df.select(\n",
    "    col('por_hacer').getItem('dia'), # Obtenemos el día\n",
    "    col('por_hacer').getItem('tareas'), # Obtenemos la lista de tareas\n",
    "    col('por_hacer').getItem('tareas').getItem(0).alias('primer_tarea') # Obtenemos el primer elemento de la lista tareas\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente vamos a convertir los datos del DF en un string JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|to_json(por_hacer)                                                       |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"dia\":\"lunes\",\"tareas\":[\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invertimos las operaciones anteriores volviendo al string del que partimos\n",
    "\n",
    "json_df.select(\n",
    "    to_json(col('por_hacer'))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funciones when, coalesce y lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|pago|\n",
      "+------+----+\n",
      "|  Jose|   1|\n",
      "| Julia|   2|\n",
      "| Katia|   1|\n",
      "|  null|   3|\n",
      "|  Raul|   3|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet('./data/c8l4/part-00000-a9b42845-6edf-4329-996e-2528aa78bb4a-c000.snappy.parquet')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si existe la necesidad de evaluar un valor frente a una lista de condiciones y devolver otro valor, entonces una solución típica es utilizar una declaración switch, que está disponible en la mayoría de los lenguajes de programación de alto nivel.\n",
    "\n",
    "Cuando sea necesario hacer esto con el valor de una columna en un DF, entonces podremos usar la función **when()** para este fin, combinado con **otherwise()** que indica el caso por defecto, a aplicar si no se cumple la condición de when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|nombre|       pago|\n",
      "+------+-----------+\n",
      "|  Jose|     pagado|\n",
      "| Julia|  sin pagar|\n",
      "| Katia|     pagado|\n",
      "|  null|sin iniciar|\n",
      "|  Raul|sin iniciar|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a sustituir los valores numéricos por diferentes strings usando when / otherwise\n",
    "\n",
    "data.select(\n",
    "    col('nombre'),\n",
    "    when(col('pago') == 1, 'pagado').when(col('pago') == 2, 'sin pagar').otherwise('sin iniciar').alias('pago')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las formas de manejar nulos es convertirlos a valores que representen nulos en nuestra lógica de procesamiento de datos.\\\n",
    "La función **coalesce()** toma uno o más valores de columna y devuelve el primero que no es nulo. Cada argumento en la función **coalesce()** debe ser de tipo columna, por lo que si se desea completar algún valor que no aparece, debemos utilizar la función lit() o literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    nombre|\n",
      "+----------+\n",
      "|      Jose|\n",
      "|     Julia|\n",
      "|     Katia|\n",
      "|sin nombre|\n",
      "|      Raul|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    coalesce(col('nombre'), lit('sin nombre')).alias('nombre') # Sustituimos el null por el string 'sin nombre'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Funciones definidas por el usuario (UDF)\n",
    "\n",
    "Tenemos cuatro opciones para crear las UDFs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. Registrar el UDF con la función register()\n",
    "\n",
    "A **register()** le pasamos el nombre con el que vamos a llamar a la función, el nombre de la función que queremos registar y por último le proporcionamos el tipo de dato que devolverá la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que retorne el cubo de un número\n",
    "\n",
    "def f_cubo(n):\n",
    "    return n * n * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.f_cubo(n)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registramos la función\n",
    "\n",
    "spark.udf.register('cubo', f_cubo, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear una vista temporal de un range, con createOrReplaceTempView(<nombre de la vista>)\n",
    "\n",
    "spark.range(1,10).createOrReplaceTempView('df_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|cubo|\n",
      "+---+----+\n",
      "|  1|   1|\n",
      "|  2|   8|\n",
      "|  3|  27|\n",
      "|  4|  64|\n",
      "|  5| 125|\n",
      "|  6| 216|\n",
      "|  7| 343|\n",
      "|  8| 512|\n",
      "|  9| 729|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Le pasamos una consulta SQL. Hacemos un select de la columna id y usamos la función cubo, para que obtenga el cubo de la columna id, \n",
    "# renombrado como 'cubo'; finalmente le indicamos que tiene que hacer la consulta desde la vista temporal que acabamos de crear\n",
    "\n",
    "spark.sql(\"SELECT id, cubo(id) AS cubo FROM df_temp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. Crear el UDF con la funcion udf()\n",
    "\n",
    "A la función udf() le pasamos como primer parámetro una función y como segundo parámetro le decimos el tipo de dato que va a retornar la UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bienvenida(nombre):\n",
    "    return ('Hola {}'.format(nombre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A la función udf() le pasamos como primer parámetro una función, en este caso una lambda a la que le pasamos la función que creamos nosotros,\n",
    "# y como segundo parámetro le decimos el tipo de dato que va a retornar la UDF\n",
    "\n",
    "bienvenida_udf = udf(lambda x: bienvenida(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|nombre|\n",
      "+------+\n",
      "|  Jose|\n",
      "| Julia|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creamos un DF\n",
    "\n",
    "df_nombre = spark.createDataFrame([('Jose',), ('Julia',)], ['nombre'])\n",
    "df_nombre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|nombre|bie_nombre|\n",
      "+------+----------+\n",
      "|  Jose| Hola Jose|\n",
      "| Julia|Hola Julia|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probamos el UDF sobre el DF que acabamos de crear\n",
    "\n",
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    bienvenida_udf(col('nombre')).alias('bie_nombre')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. Notación @udf\n",
    "\n",
    "A @udf le pasamos el tipo de dato que queremos que retorne la UDF e inmediatamente seguido definimos la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una UDF llamado mayuscula que pasa un string a mayúsculas\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def mayuscula(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|nombre|may_nombre|\n",
      "+------+----------+\n",
      "|  Jose|      JOSE|\n",
      "| Julia|     JULIA|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usamos el UDF mayúscula\n",
    "\n",
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    mayuscula(col('nombre')).alias('may_nombre')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-4. Pandas UDF\n",
    "\n",
    "Antes, u no de los problemas predominantes con el uso de las UDFs en pyspark era que tenían un rendimiento más lento que las UDFs en el lenguaje scala Esto se debía a que las UDFs de pyspark requerían el movimiento de datos entre la máquina virtual de Java y Python, lo cual era bastante costoso.\\\n",
    "Para resolver este problema se introdujeron las UDFs de pandas o UDFs vectorizadas.\n",
    "\n",
    "Para crear UDFs con pandas_udf, primero tenemos que crear una función indicando que recibe y devuelve datos tipo pandas.Series, y después le pasamos la función recién creada a pandas_udf junto con el tipo de dato que queremos que nos devuelva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que devuelva el cubo de un número, pero como tipo de dato recibe y devuelve pandas.Series\n",
    "\n",
    "def cubo_pandas(a: pd.Series) -> pd.Series: # Le decimos que recibe y devuelve datos tipo pandas.Series\n",
    "    return a * a * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pandas_udf pasándole la función que acabamos de crear y el tipo de dato que queremos que devuelva\n",
    "\n",
    "cubo_udf = pandas_udf(cubo_pandas, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una serie de pandas para probar la pandas UDF\n",
    "\n",
    "x = pd.Series([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Probamos nuestro función normal\n",
    "\n",
    "print(cubo_pandas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DF para probar la UDF\n",
    "\n",
    "df_probar_pandasUDF = spark.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_probar_pandasUDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|cubo_pandas|\n",
      "+---+-----------+\n",
      "|  0|          0|\n",
      "|  1|          1|\n",
      "|  2|          8|\n",
      "|  3|         27|\n",
      "|  4|         64|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_probar_pandasUDF.select(\n",
    "    col('id'),\n",
    "    cubo_udf(col('id')).alias('cubo_pandas')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Funciones de ventana\n",
    "\n",
    "Las funciones de ventana en Spark operan en un grupo de filas y devuelven un valor único para cada fila de entrada.\\\n",
    "Con estas funciones podemos crear grupos y luego operar sobre estos grupos.\\ \n",
    "Para realizar una operación en un grupo primero necesitamos particionar los datos utilizando la clase Window y dentro de la clase window la función partition_by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+\n",
      "| nombre|edad|departamento|evaluacion|\n",
      "+-------+----+------------+----------+\n",
      "| Lazaro|  45|      letras|        98|\n",
      "|   Raul|  24|  matemática|        76|\n",
      "|  Maria|  34|  matemática|        27|\n",
      "|   Jose|  30|     química|        78|\n",
      "| Susana|  51|     química|        98|\n",
      "|   Juan|  44|      letras|        89|\n",
      "|  Julia|  55|      letras|        92|\n",
      "|  Kadir|  38|arquitectura|        39|\n",
      "| Lilian|  23|arquitectura|        94|\n",
      "|   Rosa|  26|      letras|        91|\n",
      "|   Aian|  50|  matemática|        73|\n",
      "|Yaneisy|  29|      letras|        89|\n",
      "|Enrique|  40|     química|        92|\n",
      "|    Jon|  25|arquitectura|        78|\n",
      "|  Luisa|  39|arquitectura|        94|\n",
      "+-------+----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que en este set de datos nos solicitan obtener los trabajadores con la evaluación más alta por departamento.\n",
    "# Si se fijan, existen varios trabajadores por cada uno de los departamentos.\n",
    "\n",
    "df_ventana = spark.read.parquet('./data/funciones_ventana.parquet')\n",
    "df_ventana.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Además de la window partition, hacemos un orderBy para poder después utilizar las funciones row_number, rank y dense_rank\n",
    "# Perticionamos por departamento para que nos cree los grupos que necesitamos\n",
    "\n",
    "windowSpec = Window.partitionBy('departamento').orderBy(desc('evaluacion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. row_number\n",
    "\n",
    "Se usa para dar el número de fila secuencial, comenzando desde 1 hasta el resultado de cada partición de ventana. Se aplica sobre una especificación de ventana que hayamos creado previamente y lo hacemos con la función over()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|row_number|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         2|\n",
      "|    Jon|  25|arquitectura|        78|         3|\n",
      "|  Kadir|  38|arquitectura|        39|         4|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Rosa|  26|      letras|        91|         3|\n",
      "|   Juan|  44|      letras|        89|         4|\n",
      "|Yaneisy|  29|      letras|        89|         5|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "|  Maria|  34|  matemática|        27|         3|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "|   Jose|  30|     química|        78|         3|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora numeraremos las filas por departamento, como le especificamos en la window partition, atendiendo a la evaluación de mayor a menor\n",
    "\n",
    "df_ventana.withColumn('row_number', row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|row_number|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         2|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Y ahora aplicamos a lo anterior un filter para obtener sólo los dos trabajadores con mejor evaluación de cada departamento\n",
    "\n",
    "df_ventana.withColumn('row_number', row_number().over(windowSpec)).filter(col('row_number').isin(1,2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. rank \n",
    "\n",
    "Se usa para proporcionar un rango al resultado dentro de una partición de ventana. La diferencia con **row_number** es que esta función deja huecos en el rango cuando hay empates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----+\n",
      "| nombre|edad|departamento|evaluacion|rank|\n",
      "+-------+----+------------+----------+----+\n",
      "| Lilian|  23|arquitectura|        94|   1|\n",
      "|  Luisa|  39|arquitectura|        94|   1|\n",
      "|    Jon|  25|arquitectura|        78|   3|\n",
      "|  Kadir|  38|arquitectura|        39|   4|\n",
      "| Lazaro|  45|      letras|        98|   1|\n",
      "|  Julia|  55|      letras|        92|   2|\n",
      "|   Rosa|  26|      letras|        91|   3|\n",
      "|   Juan|  44|      letras|        89|   4|\n",
      "|Yaneisy|  29|      letras|        89|   4|\n",
      "|   Raul|  24|  matemática|        76|   1|\n",
      "|   Aian|  50|  matemática|        73|   2|\n",
      "|  Maria|  34|  matemática|        27|   3|\n",
      "| Susana|  51|     química|        98|   1|\n",
      "|Enrique|  40|     química|        92|   2|\n",
      "|   Jose|  30|     química|        78|   3|\n",
      "+-------+----+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank da el mismo número a los empates, pero lo tiene en cuenta y el siguiente valor no empatado no lo numera consecutivo, \n",
    "# por ejemplo, tenemos dos 1 y el siguiente es un 3\n",
    "\n",
    "df_ventana.withColumn('rank', rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. dense_rank\n",
    "\n",
    "Hace lo mismo que **rank**, pero sin dejar huecos cuando hay empate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|dense_rank|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         1|\n",
      "|    Jon|  25|arquitectura|        78|         2|\n",
      "|  Kadir|  38|arquitectura|        39|         3|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Rosa|  26|      letras|        91|         3|\n",
      "|   Juan|  44|      letras|        89|         4|\n",
      "|Yaneisy|  29|      letras|        89|         4|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "|  Maria|  34|  matemática|        27|         3|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "|   Jose|  30|     química|        78|         3|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dense_rank da el mismo número a los empates y el siguiente lo numera consecutivo. Aquí vemos que da dos 1 y el siguiente valor es un 2\n",
    "\n",
    "df_ventana.withColumn('dense_rank', dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-4. Agregaciones con especificaciones de ventana\n",
    "\n",
    "Cuando trabajamos con funciones de agregación no es necesario utilizar la cláusula orderBy cuando creamos la especificación de ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva especificación de ventana sin el orderBy\n",
    "\n",
    "windowSpecAgg = Window.partitionBy('departamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "| nombre|edad|departamento|evaluacion|min|max|               avg|row_number|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "| Lilian|  23|arquitectura|        94| 39| 94|             76.25|         1|\n",
      "|  Luisa|  39|arquitectura|        94| 39| 94|             76.25|         2|\n",
      "|    Jon|  25|arquitectura|        78| 39| 94|             76.25|         3|\n",
      "|  Kadir|  38|arquitectura|        39| 39| 94|             76.25|         4|\n",
      "| Lazaro|  45|      letras|        98| 89| 98|              91.8|         1|\n",
      "|  Julia|  55|      letras|        92| 89| 98|              91.8|         2|\n",
      "|   Rosa|  26|      letras|        91| 89| 98|              91.8|         3|\n",
      "|   Juan|  44|      letras|        89| 89| 98|              91.8|         4|\n",
      "|Yaneisy|  29|      letras|        89| 89| 98|              91.8|         5|\n",
      "|   Raul|  24|  matemática|        76| 27| 76|58.666666666666664|         1|\n",
      "|   Aian|  50|  matemática|        73| 27| 76|58.666666666666664|         2|\n",
      "|  Maria|  34|  matemática|        27| 27| 76|58.666666666666664|         3|\n",
      "| Susana|  51|     química|        98| 78| 98| 89.33333333333333|         1|\n",
      "|Enrique|  40|     química|        92| 78| 98| 89.33333333333333|         2|\n",
      "|   Jose|  30|     química|        78| 78| 98| 89.33333333333333|         3|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a combinar las dos especificaciones de ventana que hemos creado\n",
    "\n",
    "(df_ventana.withColumn('min', min(col('evaluacion')).over(windowSpecAgg)) # Calcula el min sobre cada partición creada con windowSpecAgg\n",
    ".withColumn('max', max(col('evaluacion')).over(windowSpecAgg)) # Calcula el max sobre cada partición creada con windowSpecAgg\n",
    ".withColumn('avg', avg(col('evaluacion')).over(windowSpecAgg)) # Calcula el avg sobre cada partición creada con windowSpecAgg\n",
    ".withColumn('row_number', row_number().over(windowSpec)) # Para aplicar row_number tenemos que usar la primera windowSpec, que tenía orderBy\n",
    " ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Catalyst Optimizer\n",
    "\n",
    ">\"La forma más fácil de escribir aplicaciones de procesamiento de datos eficiente es no preocuparse por ello y optimizar automáticamente sus aplicaciones de procesamiento de datos.\"\n",
    "\n",
    "Esta es la promesa de SparkCatalyst, que es un optimizador de consultas y es el segundo componente principal del módulo sparkSQL.\\\n",
    "Este desempeña un papel importante para garantizar que la lógica de procesamiento de datos escrita en las APIs de Data Frame o SQL se ejecute de manera eficiente y rápida.\\\n",
    "Fue diseñado para minimizar los tiempos de respuesta de las consultas de un extremo a otro, así como para que sea extensible, de modo que los usuarios de Spark puedan inyectar código de usuario en el optimizador para realizar una optimización personalizada.\n",
    "\n",
    "En alto nivel SparkCatalyst traduce la lógica de procesamiento de datos escrita por el usuario en un plan lógico, luego la optimiza utilizando heurística y finalmente convierte el plan lógico en un plan físico. El paso final es generar código basado en el plan físico.\n",
    "\n",
    "* **Plan Lógico**\n",
    "\n",
    "    1. El primer paso en el proceso de optimización de Catalyst es crear un plan lógico a partir de un objeto DF o del árbol de sintaxis abstracta de la consulta SQL analizada. El plan lógico es una representación interna de la lógica de procesamiento de datos del usuario en forma de árbol de operaciones y expresión.\n",
    "\n",
    "    2. A continuación, Catalyst analiza el plan lógico para resolver las referencias y asegurarse de que sean válidas.\n",
    "\n",
    "    3. Luego aplica al plan lógico un conjunto de optimizaciones basadas en reglas y en costos de procesamiento. Ambos tipos de optimización siguen el principio de podar los datos innecesarios lo antes posible y minimizar el costo por operador. Por ejemplo, durante esta fase de optimización, Catalyst puede decidir mover la condición del filtro antes de realizar una unión.\n",
    "\n",
    "* **Plan Físico**\n",
    "\n",
    "    1. Una vez que se optimiza el plan lógico Catalyst generará uno o más planes físicos utilizando los operadores físicos que coinciden con el motor de ejecución Spark.\n",
    "\n",
    "    2. Además de las optimizaciones realizadas en la fase del plan lógico, la fase del plan físico realiza sus propias optimizaciones basadas en reglas. Estas optimizaciones siguen el mismo principio de poda de datos descrito anteriormente en el punto 3 del plan lógico.\n",
    "\n",
    "    3. El último paso que realiza Catalyst es generar el código de bytes de Java del plan físico más económico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vuelos = spark.read.parquet('./data/flights/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|    1|  1|          4|     AS|           98|     N407AS|           ANC|                SEA|                  5|          2354|            -11|      21|        15|           205|         194|     169|    1448|      404|      4|              430|         408|          -22|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|         2336|     N3KUAA|           LAX|                PBI|                 10|             2|             -8|      12|        14|           280|         279|     263|    2330|      737|      4|              750|         741|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     US|          840|     N171US|           SFO|                CLT|                 20|            18|             -2|      16|        34|           286|         293|     266|    2296|      800|     11|              806|         811|            5|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|          258|     N3HYAA|           LAX|                MIA|                 20|            15|             -5|      15|        30|           285|         281|     258|    2342|      748|      8|              805|         756|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AS|          135|     N527AS|           SEA|                ANC|                 25|            24|             -1|      11|        35|           235|         215|     199|    1448|      254|      5|              320|         259|          -21|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/26 18:36:32 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_vuelos.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a realizar una serie de transformaciones al DF para ver después en qué orden las ha ejecutado Catalyst\n",
    "\n",
    "df_vuelos_nuevo = (df_vuelos.filter(col('MONTH').isin(6,7,8)) # Un filtrado para algunos de los meses\n",
    "            .withColumn('dis_tiempo_aire', col('DISTANCE') / col('AIR_TIME')) # Agregamos una nueva columna\n",
    ").select(\n",
    "    col('AIRLINE'), # Nos quedamos con un par de columnas\n",
    "    col('dis_tiempo_aire')\n",
    ").where(col('AIRLINE').isin('AA', 'DL', 'AS')) # Y finalmente seleccionamos con where algunas aerolíneas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder visualizar tanto el **plan lógico** como el **plan físico**, utilizaremos la función explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter 'AIRLINE IN (AA,DL,AS)\n",
      "+- Project [AIRLINE#946, dis_tiempo_aire#1005]\n",
      "   +- Project [YEAR#942, MONTH#943, DAY#944, DAY_OF_WEEK#945, AIRLINE#946, FLIGHT_NUMBER#947, TAIL_NUMBER#948, ORIGIN_AIRPORT#949, DESTINATION_AIRPORT#950, SCHEDULED_DEPARTURE#951, DEPARTURE_TIME#952, DEPARTURE_DELAY#953, TAXI_OUT#954, WHEELS_OFF#955, SCHEDULED_TIME#956, ELAPSED_TIME#957, AIR_TIME#958, DISTANCE#959, WHEELS_ON#960, TAXI_IN#961, SCHEDULED_ARRIVAL#962, ARRIVAL_TIME#963, ARRIVAL_DELAY#964, DIVERTED#965, ... 8 more fields]\n",
      "      +- Filter MONTH#943 IN (6,7,8)\n",
      "         +- Relation [YEAR#942,MONTH#943,DAY#944,DAY_OF_WEEK#945,AIRLINE#946,FLIGHT_NUMBER#947,TAIL_NUMBER#948,ORIGIN_AIRPORT#949,DESTINATION_AIRPORT#950,SCHEDULED_DEPARTURE#951,DEPARTURE_TIME#952,DEPARTURE_DELAY#953,TAXI_OUT#954,WHEELS_OFF#955,SCHEDULED_TIME#956,ELAPSED_TIME#957,AIR_TIME#958,DISTANCE#959,WHEELS_ON#960,TAXI_IN#961,SCHEDULED_ARRIVAL#962,ARRIVAL_TIME#963,ARRIVAL_DELAY#964,DIVERTED#965,... 7 more fields] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, dis_tiempo_aire: double\n",
      "Filter AIRLINE#946 IN (AA,DL,AS)\n",
      "+- Project [AIRLINE#946, dis_tiempo_aire#1005]\n",
      "   +- Project [YEAR#942, MONTH#943, DAY#944, DAY_OF_WEEK#945, AIRLINE#946, FLIGHT_NUMBER#947, TAIL_NUMBER#948, ORIGIN_AIRPORT#949, DESTINATION_AIRPORT#950, SCHEDULED_DEPARTURE#951, DEPARTURE_TIME#952, DEPARTURE_DELAY#953, TAXI_OUT#954, WHEELS_OFF#955, SCHEDULED_TIME#956, ELAPSED_TIME#957, AIR_TIME#958, DISTANCE#959, WHEELS_ON#960, TAXI_IN#961, SCHEDULED_ARRIVAL#962, ARRIVAL_TIME#963, ARRIVAL_DELAY#964, DIVERTED#965, ... 8 more fields]\n",
      "      +- Filter MONTH#943 IN (6,7,8)\n",
      "         +- Relation [YEAR#942,MONTH#943,DAY#944,DAY_OF_WEEK#945,AIRLINE#946,FLIGHT_NUMBER#947,TAIL_NUMBER#948,ORIGIN_AIRPORT#949,DESTINATION_AIRPORT#950,SCHEDULED_DEPARTURE#951,DEPARTURE_TIME#952,DEPARTURE_DELAY#953,TAXI_OUT#954,WHEELS_OFF#955,SCHEDULED_TIME#956,ELAPSED_TIME#957,AIR_TIME#958,DISTANCE#959,WHEELS_ON#960,TAXI_IN#961,SCHEDULED_ARRIVAL#962,ARRIVAL_TIME#963,ARRIVAL_DELAY#964,DIVERTED#965,... 7 more fields] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [AIRLINE#946, (cast(DISTANCE#959 as double) / cast(AIR_TIME#958 as double)) AS dis_tiempo_aire#1005]\n",
      "+- Filter (MONTH#943 IN (6,7,8) AND AIRLINE#946 IN (AA,DL,AS))\n",
      "   +- Relation [YEAR#942,MONTH#943,DAY#944,DAY_OF_WEEK#945,AIRLINE#946,FLIGHT_NUMBER#947,TAIL_NUMBER#948,ORIGIN_AIRPORT#949,DESTINATION_AIRPORT#950,SCHEDULED_DEPARTURE#951,DEPARTURE_TIME#952,DEPARTURE_DELAY#953,TAXI_OUT#954,WHEELS_OFF#955,SCHEDULED_TIME#956,ELAPSED_TIME#957,AIR_TIME#958,DISTANCE#959,WHEELS_ON#960,TAXI_IN#961,SCHEDULED_ARRIVAL#962,ARRIVAL_TIME#963,ARRIVAL_DELAY#964,DIVERTED#965,... 7 more fields] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [AIRLINE#946, (cast(DISTANCE#959 as double) / cast(AIR_TIME#958 as double)) AS dis_tiempo_aire#1005]\n",
      "+- *(1) Filter (MONTH#943 IN (6,7,8) AND AIRLINE#946 IN (AA,DL,AS))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [MONTH#943,AIRLINE#946,AIR_TIME#958,DISTANCE#959] Batched: true, DataFilters: [MONTH#943 IN (6,7,8), AIRLINE#946 IN (AA,DL,AS)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/bluetab/Documents/scala/pyspark/data/flights/vuelos.parquet], PartitionFilters: [], PushedFilters: [In(MONTH, [6,7,8]), In(AIRLINE, [AA,AS,DL])], ReadSchema: struct<MONTH:int,AIRLINE:string,AIR_TIME:int,DISTANCE:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a visualizar los planes lógico y físico con la función explain() con el parámetro True para que nos muestre ambos\n",
    "\n",
    "df_vuelos_nuevo.explain(True)\n",
    "\n",
    "# En el plan lógico optimizado se combinan el filtro del mes con el de la aerolínea: +- Filter (MONTH#943 IN (6,7,8) AND AIRLINE#946 IN (AA,DL,AS))\n",
    "# En el plan física se podan los datos innecesarios, quedándose sólo con la información necesaria para la consulta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
