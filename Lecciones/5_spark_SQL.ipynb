{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/27 09:17:24 WARN Utils: Your hostname, RNTDELL000700 resolves to a loopback address: 127.0.1.1; using 192.168.1.49 instead (on interface wlp0s20f3)\n",
      "23/07/27 09:17:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/27 09:17:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/27 09:17:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "* En Spark 1.6 se introdujo una nueva abstracción de programación llamada API estructurada. Esta es la forma preferida para realizar el procesamiento de datos en la mayoría de los casos de uso.\n",
    "\n",
    "* En esta nueva forma de hacer el procesamiento de datos, los datos deben organizarse en un formato estructurado y la lógica de cálculo de datos debe seguir una determinada estructura. Con estas dos piezas de información, Spark puede realizar optimizaciones para acelerar las aplicaciones de procesamiento de datos.\n",
    "\n",
    "* El componente SparkSQL está construido sobre el viejo y confiable componente SparkCore. Esta arquitectura en capas significa que cualquier mejora en el componente Spark Core estará disponible automáticamente para el componente para SQL.\n",
    "\n",
    "* El concepto DF se inspiró en el concepto de pandasDF de python. La principal diferencia es que un DF en Spark puede manejar un gran volumen de datos que se distribuyen en muchas máquinas.\n",
    "\n",
    "* Un concepto fundamental que diferencia a los datos estructurados de los no estructurados es un **esquema** que define la estructura de los datos en forma de nombres de columna y tipos de datos asociados. El concepto de **esquema** es una parte integral de las APIs estructuradas de Spark.\n",
    "\n",
    "* Los datos estructurados a menudo se capturan en un formato determinado. Algunos de estos formatos están basados en textos y algunos de ellos están basados en binario.\n",
    "\n",
    "    * Los formatos comunes para datos de texto son CSV, XML y JSON \n",
    "    * Los formatos comunes para datos binarios son agro, parquet y ORC.\n",
    "<br>.\n",
    "* El módulo SparkSQL y facilita la lectura y escritura de datos desde y hacia cualquiera de estos formatos. Un beneficio inesperado que surge de esta versatilidad es que Spark se puede utilizar como una herramienta de conversión de formato de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación de DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Crear un DF a partir de un RDD\n",
    "\n",
    "Hay muchas formas de crear un DF, pero **siempre hay que proporcionar un esquema**, ya sea implícita o explícitamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 4),\n",
       " (3, 9),\n",
       " (4, 16),\n",
       " (5, 25),\n",
       " (6, 36),\n",
       " (7, 49),\n",
       " (8, 64),\n",
       " (9, 81)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numero: long (nullable = true)\n",
      " |-- cuadrado: long (nullable = true)\n",
      "\n",
      "+------+--------+\n",
      "|numero|cuadrado|\n",
      "+------+--------+\n",
      "|     0|       0|\n",
      "|     1|       1|\n",
      "|     2|       4|\n",
      "|     3|       9|\n",
      "|     4|      16|\n",
      "|     5|      25|\n",
      "|     6|      36|\n",
      "|     7|      49|\n",
      "|     8|      64|\n",
      "|     9|      81|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para crear un DF a parti de un RDD no tenemos más que pasarle a la función .toDF los nombres de las columnas como lista\n",
    "df = rdd.toDF(['numero', 'cuadrado'])\n",
    "\n",
    "# Podemos ver el esquema de los datos, que nos informa del tipo de datos y de si acepta nulos o no\n",
    "df.printSchema()\n",
    "\n",
    "# Podemos ver el DF\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### También podemos crear un DF a partir de un RDD definiendo nosotros el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 1: creamos el esquema a partir de las classes de pyspark.sql.types\n",
    "\n",
    "esquema1 = StructType(\n",
    "    [\n",
    "     StructField('id', IntegerType(), True),\n",
    "     StructField('nombre', StringType(), True),\n",
    "     StructField('saldo', DoubleType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 2: creamos el esquema a partir de un string\n",
    "\n",
    "esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- saldo: double (nullable = true)\n",
      "\n",
      "+---+------+-----+\n",
      "| id|nombre|saldo|\n",
      "+---+------+-----+\n",
      "|  1|  Jose| 35.5|\n",
      "|  2|Teresa| 54.3|\n",
      "|  3| Katia| 12.7|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finalmente, para crear un DF aplicando nuestro esquema, usamos la función .createDtaFrame, pasándole el RDD y el esquema\n",
    "\n",
    "df1 = spark.createDataFrame(rdd1, schema=esquema1)\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- saldo: double (nullable = true)\n",
      "\n",
      "+---+------+-----+\n",
      "| id|nombre|saldo|\n",
      "+---+------+-----+\n",
      "|  1|  Jose| 35.5|\n",
      "|  2|Teresa| 54.3|\n",
      "|  3| Katia| 12.7|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(rdd1, schema=esquema2)\n",
    "\n",
    "df1.printSchema()\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Crear un DF a partir de fuentes de datos\n",
    "\n",
    "Las dos clases principales en SparkSQL para leer y escribir datos son DataFrameReader y DataFrameWriter respectivamente.\n",
    "\n",
    "#### DataFrameReader\n",
    "\n",
    "* Una instancia de la clase DataFrameReader está disponible como read en la sesión de Spark y la podemos invocar a través de **spark.read**\n",
    "\n",
    "* El patrón común para interactuar con DataFrameReader es **spark.read.format(...).option('key', 'value').schema(...).load()**\n",
    "\n",
    "    * **.format(...)** no es opcional. Puede ser una de las fuente de datos integradas o un formato de dato personalizado\n",
    "\n",
    "        * **Formato integrado:** se puede usar un nombre corto como por ejemplo json, parquet, jdbc, orc, csv, text...\n",
    "        * **Formato personalizado:** debe proporcionar un nombre completo\n",
    "<br>.\n",
    "\n",
    "    * **.option('key', 'value')** es opcional porque DataFrameReader tiene un conjunto de opciones predeterminadas para cada formato de fuente de datos. Podemos anular estos valores predeterminados proporcionando un valor a la función option('key', 'value')\n",
    "\n",
    "    * **.schema(...)** puede ser opcional porque algunas fuentes de datos tienen el esquema incrustado dentro de los archivos de datos, podemos pensar en .parquet u .orc En estos casos el esquema se infiere automáticamente, para otros casos es posible que deba proporcionar un esquema.\n",
    "\n",
    "* Para leer los datos hay dos alternativas aplicables a todos los formatos:\n",
    "\n",
    "    * **spark.read.<extension>(\"<path>\")** por ejemplo, spark.read.json(\"/path/to/file.json\")\n",
    "    * **spark.read.format(\"<extension>\").load(\"<path>\")** por ejemplo spark.read.format(\"json\").load(\"/path/to/file.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+\n",
      "|value                                                                  |\n",
      "+-----------------------------------------------------------------------+\n",
      "|Estamos en el curso de pyspark                                         |\n",
      "|En este capítulo estamos estudiando el API SQL de Saprk                |\n",
      "|En esta sección estamos creado dataframes a partir de fuentes de datos,|\n",
      "|y en este ejemplo creamos un dataframe a partir de un texto plano      |\n",
      "+-----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DF a partir de un archivo de texto\n",
    "dftxt = spark.read.text('./data/data_DF/dataTXT.txt')\n",
    "\n",
    "dftxt.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+--------------------+--------------------+\n",
      "|        _c0|          _c1|                 _c2|                 _c3|        _c4|                 _c5|                 _c6|    _c7|   _c8|     _c9|         _c10|                _c11|             _c12|            _c13|                _c14|                _c15|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+--------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_re...|         description|\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|               False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|               False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|               False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|               False|Today we find out...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame mediante la lectura de un archivo csv\n",
    "\n",
    "dfcsv = spark.read.csv('./data/data_DF/dataCSV.csv')\n",
    "\n",
    "# Aquí mete el nombre de las columnas en la primera fila y da nombres nuevos predeterminados\n",
    "dfcsv.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
      "|d380meD0W0M|     17.14.11|I Dare You: GOING...|            nigahiga|         24|2017-11-12T18:01:...|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para que tome la primera fila como nombre de columnas:\n",
    "dfcsv1 = spark.read.option('header', 'true').csv('./data/data_DF/dataCSV.csv')\n",
    "\n",
    "dfcsv1.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+-----+\n",
      "|pais|edad|     fecha|color|\n",
      "+----+----+----------+-----+\n",
      "|  MX|  23|2021-02-21| rojo|\n",
      "|  CA|  56|2021-06-10| azul|\n",
      "|  US|  32|2020-06-02|verde|\n",
      "+----+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leer un archivo de texto con un delimitador diferente\n",
    "\n",
    "dftxt1 = spark.read.option('header', 'true').option('delimiter', '|').csv('./data/data_DF/dataTab.txt')\n",
    "\n",
    "dftxt1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+----+\n",
      "|color|edad|     fecha|pais|\n",
      "+-----+----+----------+----+\n",
      "| rojo|null|2021-02-21|  MX|\n",
      "| azul|null|2021-06-10|  CA|\n",
      "|verde|null|2020-06-02|  US|\n",
      "+-----+----+----------+----+\n",
      "\n",
      "root\n",
      " |-- color: string (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- pais: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un json proporcionando un schema\n",
    "json_schema =  StructType(\n",
    "    [\n",
    "     StructField('color', StringType(), True),\n",
    "     StructField('edad', IntegerType(), True),\n",
    "     StructField('fecha', DateType(), True),\n",
    "     StructField('pais', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dfjson = spark.read.schema(json_schema).json('./data/data_DF/dataJSON.json')\n",
    "\n",
    "dfjson.show()\n",
    "\n",
    "dfjson.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12T19:05:...|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13T11:00:...|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
      "|d380meD0W0M|     17.14.11|I Dare You: GOING...|            nigahiga|         24|2017-11-12T18:01:...|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame a partir de un archivo parquet\n",
    "dfparquet = spark.read.parquet('./data/data_DF/dataPARQUET.parquet')\n",
    "\n",
    "dfparquet.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Otra alternativa para leer desde una fuente de datos parquet en este caso\n",
    "dfparquet1 = spark.read.format('parquet').load('./data/data_DF/dataPARQUET.parquet')\n",
    "\n",
    "dfparquet1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trabajo con columnas\n",
    "\n",
    "* A diferencia de las operaciones con RDD, las operaciones estructuradas están diseñadas para ser más relacionales, lo que significa que estas operaciones reflejan el tipo de expresiones que pueden hacer con SQL como filtrado, transformación, unión, entre otras.\n",
    "\n",
    "* Al igual que las operaciones con RDD, las operaciones estructuradas se dividen en dos categorías: transformaciones y acciones. La semántica de las transformaciones y acciones estructuradas es idéntica a la de los RDD: las transformaciones estructuradas son **lazy evaluation** y las acciones estructuradas son **eager evaluation**.\n",
    "\n",
    "* Recordmeos que los DF son inmutables y sus operaciones de transformación siempre devuelven un nuevo DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- dislikes: string (nullable = true)\n",
      " |-- comment_count: string (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet2 = spark.read.parquet('./data/dataPARQUET.parquet')\n",
    "\n",
    "dfparquet2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|WE WANT TO TALK A...|\n",
      "|The Trump Preside...|\n",
      "|Racist Superman |...|\n",
      "|Nickelback Lyrics...|\n",
      "|I Dare You: GOING...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Primera alternativa para referirnos a las columnas\n",
    "\n",
    "dfparquet2.select('title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|WE WANT TO TALK A...|\n",
      "|The Trump Preside...|\n",
      "|Racist Superman |...|\n",
      "|Nickelback Lyrics...|\n",
      "|I Dare You: GOING...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Segunda alternativapara referirnos a las columnas\n",
    "\n",
    "dfparquet2.select(col('title')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3-1. Funciones select y selectExpr\n",
    "\n",
    "* **select:** esta transformación se usa comúnmente para realizar la proyección, es decir, seleccionar todas o un subconjunto de columnas de un DF. Durante la selección cada columna se puede transformar mediante una expresión de columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el parquet, muy similar al anterior, pero con alguna modificación en el tipo de datos de cada columna\n",
    "dfparquet3 = spark.read.parquet('./data/datos.parquet')\n",
    "\n",
    "dfparquet3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   video_id|\n",
      "+-----------+\n",
      "|2kyS6SvSYSE|\n",
      "|1ZAPwfrtAFY|\n",
      "|5qpjK5DgCt4|\n",
      "|puqaWrEC7tY|\n",
      "|d380meD0W0M|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.select(col('video_id')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|   video_id|trending_date|\n",
      "+-----------+-------------+\n",
      "|2kyS6SvSYSE|     17.14.11|\n",
      "|1ZAPwfrtAFY|     17.14.11|\n",
      "|5qpjK5DgCt4|     17.14.11|\n",
      "|puqaWrEC7tY|     17.14.11|\n",
      "|d380meD0W0M|     17.14.11|\n",
      "+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.select('video_id', 'trending_date').show(5)\n",
    "# El problema de esta nomenclatura es que no podemos pasarle funciones, como se puede comprobar en la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Esta expresión nos dará error\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dfparquet3\u001b[39m.\u001b[39mselect(\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlikes\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdislikes\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m----> 6\u001b[0m     (\u001b[39m'\u001b[39;49m\u001b[39mlikes\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdislikes\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m )\u001b[39m.\u001b[39mshow(\u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Esta expresión nos dará error\n",
    "\n",
    "dfparquet3.select(\n",
    "    'likes',\n",
    "    'dislikes',\n",
    "    ('likes' - 'dislikes')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "| likes|dislikes|aceptacion|\n",
      "+------+--------+----------+\n",
      "| 57527|    2966|     54561|\n",
      "| 97185|    6146|     91039|\n",
      "|146033|    5339|    140694|\n",
      "| 10172|     666|      9506|\n",
      "|132235|    1989|    130246|\n",
      "+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La forma correcta es usando la función col\n",
    "\n",
    "dfparquet3.select(\n",
    "    col('likes'),\n",
    "    col('dislikes'),\n",
    "    (col('likes') - col('dislikes')).alias('aceptacion')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **selectExpr:** nos permite aplicar una expresión a la selección de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "| likes|dislikes|aceptacion|\n",
      "+------+--------+----------+\n",
      "| 57527|    2966|     54561|\n",
      "| 97185|    6146|     91039|\n",
      "|146033|    5339|    140694|\n",
      "| 10172|     666|      9506|\n",
      "|132235|    1989|    130246|\n",
      "+------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Le pedimos los mismo que en la celda anterior\n",
    "dfparquet3.selectExpr('likes', 'dislikes', '(likes - dislikes) as aceptacion').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|videos|\n",
      "+------+\n",
      "|  6837|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contamos el número de valores diferentes dentro de la columna video_id\n",
    "dfparquet3.selectExpr(\"count(distinct(video_id)) as videos\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Como conclusión, deberíamos tener en cuenta que las expresiones SQL son construcciones poderosas y flexibles que permiten expresar la lógica de transformación de columnas de una manera natural, tal como lo podríamos estar pensando nosotros mismos. Podemos expresar expresiones SQL en un formato de cadena y spark las analizará en un árbol lógico que se evalúa en un orden correcto. Además hay que tener en cuenta que la combinación de expresiones SQL y funciones integradas facilita la realización de ciertos análisis de datos que de otro modo requerirían varios pasos.\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Funciones filter y where\n",
    "\n",
    "* **filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|       publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13 18:13:01|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13 08:30:00|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12 20:05:24|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13 12:00:04|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
      "|d380meD0W0M|     17.14.11|I Dare You: GOING...|            nigahiga|         24|2017-11-12 19:01:41|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+-------------+-----------+-------------------+---------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|channel_title|category_id|       publish_time|           tags|  views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+-------------+-----------+-------------------+---------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...| CaseyNeistat|         22|2017-11-13 18:13:01|SHANtell martin| 748374|57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|2kyS6SvSYSE|     17.15.11|WE WANT TO TALK A...| CaseyNeistat|         22|2017-11-13 18:13:01|SHANtell martin|2188590|88099|    7150|        24225|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|2kyS6SvSYSE|     17.16.11|WE WANT TO TALK A...| CaseyNeistat|         22|2017-11-13 18:13:01|SHANtell martin|2325233|91111|    7543|        21450|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|2kyS6SvSYSE|     17.17.11|WE WANT TO TALK A...| CaseyNeistat|         22|2017-11-13 18:13:01|SHANtell martin|2400741|92831|    7687|        21714|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|2kyS6SvSYSE|     17.18.11|WE WANT TO TALK A...| CaseyNeistat|         22|2017-11-13 18:13:01|SHANtell martin|2468267|94303|    7802|        21866|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|2kyS6SvSYSE|     17.19.11|WE WANT TO TALK A...| CaseyNeistat|         22|2017-11-13 18:13:01|SHANtell martin|2524854|95587|    7892|        22038|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|2kyS6SvSYSE|     17.20.11|WE WANT TO TALK A...| CaseyNeistat|         22|2017-11-13 18:13:01|SHANtell martin|2564903|96321|    7972|        22149|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "+-----------+-------------+--------------------+-------------+-----------+-------------------+---------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.filter(col('video_id') == '2kyS6SvSYSE').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **where**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|            video_id|       trending_date|               title|       channel_title|         category_id|       publish_time|                tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|\\nCook with confi...|             recipes|              videos| and restaurant g...| dining destinations|               null|                null|  null| null|    null|         null|                null|             null|            null|                  null|                null|\n",
      "|\\nVogue is the au...|      culture trends|     beauty coverage|              videos|     celebrity style|               null|                null|  null| null|    null|         null|                null|             null|            null|                  null|                null|\n",
      "|\\nWIRED is where ...| WIRED explores t...|          innovation| and culture.\\n\\n...|                null|               null|                null|  null| null|    null|         null|                null|             null|            null|                  null|                null|\n",
      "|         YvfYK0EEhK4|            17.15.11|Brent Pella - Why...|         Brent Pella|                  23|2017-11-14 16:32:51|\"spirit airlines\"...|462490|14132|     795|          666|https://i.ytimg.c...|            False|           False|                 False|Traveling for the...|\n",
      "|         cxMvzK2OQTw|            17.15.11|Cards Against Hum...|Cards Against Hum...|                  23|2017-11-14 17:43:11|              [none]|295217| 4854|    1439|         1267|https://i.ytimg.c...|            False|           False|                 False|Narrated by Peter...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet4 = spark.read.parquet('./data/datos.parquet').where(col('trending_date') != '17.14.11')\n",
    "\n",
    "dfparquet4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|       publish_time|                tags|  views| likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|        CaseyNeistat|         22|2017-11-13 18:13:01|     SHANtell martin| 748374| 57527|    2966|        15954|https://i.ytimg.c...|            False|           False|                 False|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|2017-11-13 08:30:00|\"last week tonigh...|2418783| 97185|    6146|        12703|https://i.ytimg.c...|            False|           False|                 False|One year after th...|\n",
      "|5qpjK5DgCt4|     17.14.11|Racist Superman |...|        Rudy Mancuso|         23|2017-11-12 20:05:24|\"racist superman\"...|3191434|146033|    5339|         8181|https://i.ytimg.c...|            False|           False|                 False|WATCH MY PREVIOUS...|\n",
      "|puqaWrEC7tY|     17.14.11|Nickelback Lyrics...|Good Mythical Mor...|         24|2017-11-13 12:00:04|\"rhett and link\"|...| 343168| 10172|     666|         2146|https://i.ytimg.c...|            False|           False|                 False|Today we find out...|\n",
      "|d380meD0W0M|     17.14.11|I Dare You: GOING...|            nigahiga|         24|2017-11-12 19:01:41|\"ryan\"|\"higa\"|\"hi...|2095731|132235|    1989|        17518|https://i.ytimg.c...|            False|           False|                 False|I know it's been ...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+------+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet5 = spark.read.parquet('./data/datos.parquet').where(col('likes') > 5000)\n",
    "\n",
    "dfparquet5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|       publish_time|                tags|  views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|YvfYK0EEhK4|     17.15.11|Brent Pella - Why...|         Brent Pella|         23|2017-11-14 16:32:51|\"spirit airlines\"...| 462490|14132|     795|          666|https://i.ytimg.c...|            False|           False|                 False|Traveling for the...|\n",
      "|bAkEd8r7Nnw|     17.15.11|Slow Mo Katana Sw...|    The Slow Mo Guys|         24|2017-11-14 19:31:20|\"slomo\"|\"slow\"|\"m...|1525400|63995|     896|         4697|https://i.ytimg.c...|            False|           False|                 False|Has Dan ever used...|\n",
      "|ItYOdWRo0JY|     17.15.11|Selling My iPhone...|          TechSmartt|         28|2017-11-14 01:45:15|\"iphone x\"|\"iphon...|1836419|28647|   17815|         7860|https://i.ytimg.c...|            False|           False|                 False|I saw the EcoATM ...|\n",
      "|5530I_pYjbo|     17.15.11|How I Trained My ...|         JunsKitchen|         26|2017-11-14 13:06:56|\"how\"|\"trained\"|\"...| 977285|87615|     391|         5575|https://i.ytimg.c...|            False|           False|                 False|►EQUIPMENT I use ...|\n",
      "|gjXrm2Q-te4|     17.15.11|Jimmy Fallon Pays...|The Tonight Show ...|         23|2017-11-14 05:55:24|\"Jimmy Fallon\"|\"T...|1611093|58474|     837|         3294|https://i.ytimg.c...|            False|           False|                 False|Jimmy celebrates ...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+-------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet5.filter((col('trending_date') != '17.14.11') & (col('likes') > 7000)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto da exactamente el mismo resultado que la celda anterior, pero usando filter\n",
    "dfparquet5.filter(col('trending_date') != '17.14.11').filter(col('likes') > 7000).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Funciones distinc y dropDuplicates\n",
    "\n",
    "Estas dos transformaciones tienen un comportamiento idéntico sin embargo, **dropDuplicates** nos permitirá controlar qué columnas deben usarse en la lógica de deduplicación. Si no especificamos ninguna columna, la lógica de deduplicación utilizará todas las columnas del DF.\n",
    "\n",
    "* **distinc** elimina todos los duplicados de todas las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El conteo del dataframe original es 48137\n",
      "El conteo del dataframe sin duplicados es 41428\n"
     ]
    }
   ],
   "source": [
    "df_sin_duplicados = dfparquet3.distinct()\n",
    "\n",
    "print('El conteo del dataframe original es {}'.format(dfparquet3.count()))\n",
    "print('El conteo del dataframe sin duplicados es {}'.format(df_sin_duplicados.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **dropDuplicates** elimina los duplicados en las columnas que le indiquemos en forma de lista. Si no indicamos ninguna, hace lo mismo que distinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id|color|importe|\n",
      "+---+-----+-------+\n",
      "|  1| azul|    567|\n",
      "|  2| rojo|    487|\n",
      "|  1| azul|    345|\n",
      "|  2|verde|    783|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe = spark.createDataFrame([(1, 'azul', 567), (2, 'rojo', 487), (1, 'azul', 345), (2, 'verde', 783)]).toDF('id', 'color', 'importe')\n",
    "\n",
    "dataframe .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id|color|importe|\n",
      "+---+-----+-------+\n",
      "|  1| azul|    567|\n",
      "|  2| rojo|    487|\n",
      "|  2|verde|    783|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Elimina los registros cuyas columnas id y color sean idénticas\n",
    "dataframe.dropDuplicates(['id', 'color']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Funciones sort, orderBy y limit\n",
    "\n",
    "sort y orderby tienen la misma semántica pero la transformación orderBy es más relacional que la de sort.\n",
    "De forma predeterminada, el orden está dado en forma ascendente y es bastante fácil cambiarlo a descendente.\n",
    "Al especificar más de una columna, es posible tener un orden diferente para cada una de las columnas.\n",
    "\n",
    "Con limit elegimos cuántas filas queremos mostrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------+--------+\n",
      "|likes|  views|   video_id|dislikes|\n",
      "+-----+-------+-----------+--------+\n",
      "|63995|1525400|bAkEd8r7Nnw|     896|\n",
      "|  427|   9036|eijd-yjXY9E|      14|\n",
      "| 4145| 318249|npcqBt_e4k0|     110|\n",
      "| 6669| 203615|LeWtF5y9-6Q|     136|\n",
      "| 2166| 104499|GhcqN2FDAnA|    1066|\n",
      "+-----+-------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leemos el DF con al de procesamiento incluído en la carga\n",
    "dfparquet6 = (spark.read.parquet('./data/datos.parquet')\n",
    "    .select(col('likes'), col('views'), col('video_id'), col('dislikes'))\n",
    "    .dropDuplicates(['video_id'])\n",
    ")\n",
    "\n",
    "dfparquet6.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+--------+\n",
      "|likes|views|            video_id|dislikes|\n",
      "+-----+-----+--------------------+--------+\n",
      "| null| null|\\nMust-See WWE vi...|    null|\n",
      "| null| null|Horror Outro ► ht...|    null|\n",
      "| null| null|             \\nToday|    null|\n",
      "| null| null|\\nhttp://www.Mast...|    null|\n",
      "| null| null|\\nSIGN UP FOR BRA...|    null|\n",
      "+-----+-----+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort va a ordenar el DF por la columna like de forma ascendente\n",
    "\n",
    "dfparquet6.sort('likes').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------+\n",
      "|  likes|   views|   video_id|dislikes|\n",
      "+-------+--------+-----------+--------+\n",
      "|3880071|39349927|7C2z4GqqS5E|   72707|\n",
      "|2055137|13945717|kTlv5_Bs8aw|   23888|\n",
      "|2050527|10695328|OK3GJ0WIQ8s|   14711|\n",
      "|1956202|10666323|p8npDG2ulKQ|   13966|\n",
      "|1735895|37736281|6ZfuNTqbHE8|   21969|\n",
      "+-------+--------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort va a ordenar el DF por la columna like de forma descendente\n",
    "\n",
    "dfparquet6.sort(desc('likes')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+--------+\n",
      "|likes|views|            video_id|dislikes|\n",
      "+-----+-----+--------------------+--------+\n",
      "| null| null|             \\nToday|    null|\n",
      "| null| null|\\nBIG SHINY Lens ...|    null|\n",
      "| null| null|\\nMust-See WWE vi...|    null|\n",
      "| null| null|\\nSIGN UP FOR BRA...|    null|\n",
      "| null| null|          \\nEvelin 7|    null|\n",
      "+-----+-----+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderBy\n",
    "\n",
    "dfparquet6.orderBy(col('views')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------+\n",
      "|  likes|   views|   video_id|dislikes|\n",
      "+-------+--------+-----------+--------+\n",
      "| 609101|48431654|-BQJo3vK8O8|   52259|\n",
      "|3880071|39349927|7C2z4GqqS5E|   72707|\n",
      "|1111592|38873543|i0p1bmr0EmE|   96407|\n",
      "|1735895|37736281|6ZfuNTqbHE8|   21969|\n",
      "|1634124|33523622|2Vv-BfVoq4g|   21082|\n",
      "+-------+--------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet6.orderBy(col('views').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id|color|importe|\n",
      "+---+-----+-------+\n",
      "|  1| azul|    568|\n",
      "|  2| rojo|    235|\n",
      "|  1| azul|    456|\n",
      "|  2| azul|    783|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2 = spark.createDataFrame([(1, 'azul', 568), (2, 'rojo', 235), (1, 'azul', 456), (2, 'azul', 783)]).toDF('id', 'color', 'importe')\n",
    "\n",
    "dataframe2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id|color|importe|\n",
      "+---+-----+-------+\n",
      "|  2| rojo|    235|\n",
      "|  1| azul|    456|\n",
      "|  1| azul|    568|\n",
      "|  2| azul|    783|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ahora ordenaremos la columna color en orden descendente e importe ascendente\n",
    "dataframe2.orderBy(col('color').desc(), col('importe')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------+\n",
      "|  likes|   views|   video_id|dislikes|\n",
      "+-------+--------+-----------+--------+\n",
      "| 609101|48431654|-BQJo3vK8O8|   52259|\n",
      "|3880071|39349927|7C2z4GqqS5E|   72707|\n",
      "|1111592|38873543|i0p1bmr0EmE|   96407|\n",
      "|1735895|37736281|6ZfuNTqbHE8|   21969|\n",
      "|1634124|33523622|2Vv-BfVoq4g|   21082|\n",
      "|1405355|31648454|VYOjWnS4cMY|   51547|\n",
      "| 850362|27973210|u9Mv98Gr5pY|   26541|\n",
      "|1149185|24782158|FlsCjmMhFmw|  483924|\n",
      "| 641546|24421448|U9BwWKXjVaI|   16517|\n",
      "| 587326|23758250|1J76wN0TPI4|   18799|\n",
      "+-------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit para visualizar el top 10 de videos con más visualizaciones\n",
    "\n",
    "top_10 = dfparquet6.orderBy(col('views').desc()).limit(10)\n",
    "\n",
    "top_10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+--------+\n",
      "|  likes|   views|   video_id|dislikes|\n",
      "+-------+--------+-----------+--------+\n",
      "| 835378|13305605|QwZT7T-TXT0|  629120|\n",
      "|1149185|24782158|FlsCjmMhFmw|  483924|\n",
      "|1207457|13754992|_5d-sQ7Fh5M|  280675|\n",
      "|   9100| 1142585|LFhT6H6pRWg|  218841|\n",
      "| 285349| 6886948|ooyjaVdt-jA|  164004|\n",
      "|1167488| 8041970|oWjxSkJpxFU|  147643|\n",
      "|  32892|14647590|V5cOvyDpWfM|  117128|\n",
      "|   4870|  985179|8d_202l55LU|  110707|\n",
      "|1111592|38873543|i0p1bmr0EmE|   96407|\n",
      "| 761852| 7691902|khPLWaBioOs|   86475|\n",
      "+-------+--------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a ver los videos más odiados de youtube\n",
    "topmierder = dfparquet6.orderBy(col('dislikes').desc()).limit(10)\n",
    "\n",
    "topmierder.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Funciones whithColumn y withColumnRenamed\n",
    "\n",
    "* **whithColumn** se usa para agregar una nueva columna a un DF. Requiere dos parámetros de entrada, un nombre de columna y un valor en forma de expresión de columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- valoracion: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear una nueva columna con whithColumn\n",
    "df_valoracion = dfparquet3.withColumn('valoracion', col('likes') - col('dislikes'))\n",
    "\n",
    "df_valoracion.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- valoracion: integer (nullable = true)\n",
      " |-- res_div: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valoracion1 = (dfparquet3.withColumn('valoracion', col('likes') - col('dislikes'))\n",
    "                    .withColumn('res_div', col('valoracion') % 10)\n",
    ")\n",
    "\n",
    "df_valoracion1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+-------+\n",
      "| likes|dislikes|valoracion|res_div|\n",
      "+------+--------+----------+-------+\n",
      "| 57527|    2966|     54561|      1|\n",
      "| 97185|    6146|     91039|      9|\n",
      "|146033|    5339|    140694|      4|\n",
      "| 10172|     666|      9506|      6|\n",
      "|132235|    1989|    130246|      6|\n",
      "+------+--------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valoracion1.select(col('likes'), col('dislikes'), col('valoracion'), col('res_div')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **withColumnRenamed** se usa para cambiar de nombre una columna. Recibe dos parámetros, el nombre actual y el nuevo nombre. Si el nombre de la columna actual proporciobnado no existe, sprak no arroja error, simplemente no hace nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renombrado = dfparquet3.withColumnRenamed('video_id', 'id')\n",
    "\n",
    "df_renombrado.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Le pasamos un nombre incorrecto y no da error, pero no hace nada\n",
    "df_error = dfparquet3.withColumnRenamed('nombre_que_no_existe', 'otro_nombre')\n",
    "\n",
    "df_error.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Funciones drop, sample y randomSplit\n",
    "\n",
    "* **drop** elimina las columnas especificadas del data frame. Pueden especificarse uno o más nombres de columna para eliminar, pero solo se eliminarán las que existan en el esquema y las que no se ignorarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util = dfparquet3.drop('comments_disabled')\n",
    "\n",
    "df_util.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util1 = dfparquet3.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link')\n",
    "\n",
    "df_util1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aunque le pasemos una  columna que no existe, en este caso 'cafe', no arrojará error, simplemente la ignorará\n",
    "df_util = dfparquet3.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link', 'cafe')\n",
    "\n",
    "df_util.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **sample** es una transformación que devuelve un conjunto de filas seleccionados aleatoriamente del DF. El número de filas devuelto será aproximadamente igual al número especificado, que representa un porcentaje y es un valor que debe oscilar entre cero y uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_muestra = dfparquet3.sample(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El 80% de filas del dataframe original es 38509.6\n",
      "El numero de filas del dataframe muestra es 38628\n"
     ]
    }
   ],
   "source": [
    "num_filas = dfparquet3.count()\n",
    "num_filas_muestra = df_muestra.count()\n",
    "\n",
    "print('El 80% de filas del dataframe original es {}'.format(num_filas - (num_filas*0.2)))\n",
    "print('El numero de filas del dataframe muestra es {}'.format(num_filas_muestra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Aunque **sample** es un muestreo aleatorio, podemos hacer que repita siempre la misma selección si le pasamos un seed\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_muestra1 = dfparquet3.sample(fraction=0.8, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">A **sample** también podemos indicarle si queremos que tome las filas del DF con reemplazo o no\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_muestra = dfparquet3.sample(withReplacement=True, fraction=0.8, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **randomSplit** es una transformación que se usa comúnmente durante el proceso de preparación de los datos para entrenar modelos de ML.\n",
    "\n",
    "A diferencia de las transformaciones anteriores, esta nos va a devolver uno o más data frame y la cantidad de data free que devuelve se depende de la cantidad de pesos que especifiquemos. Si el conjunto de pesos proporcionados no suma uno, estos pesos se normalizan en consecuencia para sumar uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a asignar el 80% de las filas al DF train y el 20% al DF test\n",
    "train, test = dfparquet3.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38542\n",
      "9595\n"
     ]
    }
   ],
   "source": [
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a asignar el 60% de las filas al DF train1, el 20% al validation1 y el 20% al DF test1\n",
    "train1, validation1, test1 = dfparquet3.randomSplit([0.6, 0.2, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28799\n",
      "9743\n",
      "9595\n"
     ]
    }
   ],
   "source": [
    "print(train1.count())\n",
    "print(validation1.count())\n",
    "print(test1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trabajo con datos incorrectos o faltantes\n",
    "\n",
    "* Los datos con los que trabajamos a menudo no están tan limpio como nos gustaría. En parte esto se debe a que los datos evolucionan con el tiempo y por lo tanto algunas columnas tienen valores y otras no. Es importante abordar este tipo de problema al comienzo de la lógica de la manipulación de datos para evitar sorpresas desagradables que harán que su trabajo de procesamiento de datos de larga duración deje de funcionar.\n",
    "\n",
    "* La comunidad de spark reconoce que la necesidad de lidiar con datos faltantes es una realidad. Por lo tanto, proporciona una clase dedicada para ayudar a afrontar este problema.\n",
    "\n",
    "### Formas más habituales de tratar los datos faltantes\n",
    "\n",
    "* Eliminar las filas que tienen valores perdidos en una o más columnas.\n",
    "\n",
    "* Llenar esos valores faltantes con valores proporcionados por el usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. Eliminar nulos\n",
    "\n",
    "* Podemos eliminar **todos** los valores nulos del DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48137"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primero vamos a ver el número de registros del DF\n",
    "dfparquet3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40379"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora vamos a eliminar todos los registros que tengan al menos un valor nulo y ver cuánto registros quedan\n",
    "dfparquet3.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40379"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos hacer lo mismo que en la celda anterior de este modo:\n",
    "dfparquet3.na.drop('any').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40379"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y aún hay otro método para eliminar nulos:\n",
    "dfparquet3.dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos eliminar **algunos** de los valores nulos del DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40949"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a eliminar filas que tengan nulos sólo en la columna 'views'\n",
    "dfparquet3.na.drop(subset=['views']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40949"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a eliminar filas que tengan nulos sólo en las columna 'views' y 'dislikes'\n",
    "dfparquet3.na.drop(subset=['views', 'dislikes']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Rellenar los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "|views|likes|dislikes|\n",
      "+-----+-----+--------+\n",
      "| null| null|    null|\n",
      "| null| null|    null|\n",
      "| null| null|    null|\n",
      "| null| null|    null|\n",
      "| null| null|    null|\n",
      "+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vemos que hay tres columnas llenas de nulos\n",
    "dfparquet3.orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos rellenar **todos** los nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "|views|likes|dislikes|\n",
      "+-----+-----+--------+\n",
      "|    0|    0|       0|\n",
      "|    0|    0|       0|\n",
      "|    0|    0|       0|\n",
      "|    0|    0|       0|\n",
      "|    0|    0|       0|\n",
      "+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sustituímos los todos los nulos de las columnas seleccionadas por 0 con fillna\n",
    "dfparquet3.fillna(0).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos rellenar **algunos** nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "|views|likes|dislikes|\n",
      "+-----+-----+--------+\n",
      "| null|    0|       0|\n",
      "| null|    0|       0|\n",
      "| null|    0|       0|\n",
      "| null|    0|       0|\n",
      "| null|    0|       0|\n",
      "+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sustituímos por 0 sólo los nulos de las columnas 'likes' y 'dislikes'\n",
    "dfparquet3.fillna(0, subset=['likes', 'dislikes']).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Acciones sobre un DF en SparkSQL\n",
    "\n",
    "*  Estas acciones tienen el mismo comportamiento que las acciones realizadas en un RDD, es decir, son **eager evaluation**, por lo que desencadenarán el cálculo de todas las transformaciones acumuladas en el DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. show()\n",
    "Por defecto nos da las primeras 20 filas de un DF y trunca el contenido si supera el ancho máximo de columna por defecto, pero podemos indicarle tanto el número de filas que queremos ver, como que las queremos completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------------------------------------------------+---------------------+-----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------+--------+-------------+----------------------------------------------+-----------------+----------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|video_id   |trending_date|title                                                         |channel_title        |category_id|publish_time       |tags                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |views  |likes |dislikes|comment_count|thumbnail_link                                |comments_disabled|ratings_disabled|video_error_or_removed|description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-----------+-------------+--------------------------------------------------------------+---------------------+-----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------+--------+-------------+----------------------------------------------+-----------------+----------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2kyS6SvSYSE|17.14.11     |WE WANT TO TALK ABOUT OUR MARRIAGE                            |CaseyNeistat         |22         |2017-11-13 18:13:01|SHANtell martin                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |748374 |57527 |2966    |15954        |https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg|False            |False           |False                 |SHANTELL'S CHANNEL - https://www.youtube.com/shantellmartin\\nCANDICE - https://www.lovebilly.com\\n\\nfilmed this video in 4k on this -- http://amzn.to/2sTDnRZ\\nwith this lens -- http://amzn.to/2rUJOmD\\nbig drone - http://tinyurl.com/h4ft3oy\\nOTHER GEAR ---  http://amzn.to/2o3GLX5\\nSony CAMERA http://amzn.to/2nOBmnv\\nOLD CAMERA; http://amzn.to/2o2cQBT\\nMAIN LENS; http://amzn.to/2od5gBJ\\nBIG SONY CAMERA; http://amzn.to/2nrdJRO\\nBIG Canon CAMERA; http://tinyurl.com/jn4q4vz\\nBENDY TRIPOD THING; http://tinyurl.com/gw3ylz2\\nYOU NEED THIS FOR THE BENDY TRIPOD; http://tinyurl.com/j8mzzua\\nWIDE LENS; http://tinyurl.com/jkfcm8t\\nMORE EXPENSIVE WIDE LENS; http://tinyurl.com/zrdgtou\\nSMALL CAMERA; http://tinyurl.com/hrrzhor\\nMICROPHONE; http://tinyurl.com/zefm4jy\\nOTHER MICROPHONE; http://tinyurl.com/jxgpj86\\nOLD DRONE (cheaper but still great);http://tinyurl.com/zcfmnmd\\n\\nfollow me; on http://instagram.com/caseyneistat\\non https://www.facebook.com/cneistat\\non https://twitter.com/CaseyNeistat\\n\\namazing intro song by https://soundcloud.com/discoteeth\\n\\nad disclosure.  THIS IS NOT AN AD.  not selling or promoting anything.  but samsung did produce the Shantell Video as a 'GALAXY PROJECT' which is an initiative that enables creators like Shantell and me to make projects we might otherwise not have the opportunity to make.  hope that's clear.  if not ask in the comments and i'll answer any specifics.|\n",
      "|1ZAPwfrtAFY|17.14.11     |The Trump Presidency: Last Week Tonight with John Oliver (HBO)|LastWeekTonight      |24         |2017-11-13 08:30:00|\"last week tonight trump presidency\"|\"last week tonight donald trump\"|\"john oliver trump\"|\"donald trump\"                                                                                                                                                                                                                                                                                                                                                                                       |2418783|97185 |6146    |12703        |https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg|False            |False           |False                 |One year after the presidential election, John Oliver discusses what we've learned so far and enlists our catheter cowboy to teach Donald Trump what he hasn't.\\n\\nConnect with Last Week Tonight online...\\n\\nSubscribe to the Last Week Tonight YouTube channel for more almost news as it almost happens: www.youtube.com/user/LastWeekTonight\\n\\nFind Last Week Tonight on Facebook like your mom would: http://Facebook.com/LastWeekTonight\\n\\nFollow us on Twitter for news about jokes and jokes about news: http://Twitter.com/LastWeekTonight\\n\\nVisit our official site for all that other stuff at once: http://www.hbo.com/lastweektonight                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|5qpjK5DgCt4|17.14.11     |Racist Superman | Rudy Mancuso, King Bach & Lele Pons         |Rudy Mancuso         |23         |2017-11-12 20:05:24|\"racist superman\"|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"|\"racist\"|\"superman\"|\"love\"|\"rudy mancuso poo bear black white official music video\"|\"iphone x by pineapple\"|\"lelepons\"|\"hannahstocking\"|\"rudymancuso\"|\"inanna\"|\"anwar\"|\"sarkis\"|\"shots\"|\"shotsstudios\"|\"alesso\"|\"anitta\"|\"brazil\"|\"Getting My Driver's License | Lele Pons\"                                                                                                                                                                   |3191434|146033|5339    |8181         |https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg|False            |False           |False                 |WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► https://www.youtube.com/channel/UC5jkXpfnBhlDjqh0ir5FsIQ?sub_confirmation=1\\n\\nTHANKS FOR WATCHING! LIKE & SUBSCRIBE FOR MORE VIDEOS!\\n-----------------------------------------------------------\\nFIND ME ON: \\nInstagram | http://instagram.com/rudymancuso\\nTwitter | http://twitter.com/rudymancuso\\nFacebook | http://facebook.com/rudymancuso\\n\\nCAST: \\nRudy Mancuso | http://youtube.com/c/rudymancuso\\nLele Pons | http://youtube.com/c/lelepons\\nKing Bach | https://youtube.com/user/BachelorsPadTv\\n\\nVideo Effects: \\nCaleb Natale | https://instagram.com/calebnatale\\n\\nPA:\\nPaulina Gregory\\n\\n\\nShots Studios Channels:\\nAlesso | https://youtube.com/c/alesso\\nAnitta | http://youtube.com/c/anitta\\nAnwar Jibawi | http://youtube.com/c/anwar\\nAwkward Puppets | http://youtube.com/c/awkwardpuppets\\nHannah Stocking | http://youtube.com/c/hannahstocking\\nInanna Sarkis | http://youtube.com/c/inanna\\nLele Pons | http://youtube.com/c/lelepons\\nMaejor | http://youtube.com/c/maejor\\nMike Tyson | http://youtube.com/c/miketyson \\nRudy Mancuso | http://youtube.com/c/rudymancuso\\nShots Studios | http://youtube.com/c/shots\\n\\n#Rudy\\n#RudyMancuso                                                                                                                                                                                                                                         |\n",
      "|puqaWrEC7tY|17.14.11     |Nickelback Lyrics: Real or Fake?                              |Good Mythical Morning|24         |2017-11-13 12:00:04|\"rhett and link\"|\"gmm\"|\"good mythical morning\"|\"rhett and link good mythical morning\"|\"good mythical morning rhett and link\"|\"mythical morning\"|\"Season 12\"|\"nickelback lyrics\"|\"nickelback lyrics real or fake\"|\"nickelback\"|\"nickelback songs\"|\"nickelback song\"|\"rhett link nickelback\"|\"gmm nickelback\"|\"lyrics (website category)\"|\"nickelback (musical group)\"|\"rock\"|\"music\"|\"lyrics\"|\"chad kroeger\"|\"canada\"|\"music (industry)\"|\"mythical\"|\"gmm challenge\"|\"comedy\"|\"funny\"|\"challenge\"|343168 |10172 |666     |2146         |https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg|False            |False           |False                 |Today we find out if Link is a Nickelback amateur or a secret Nickelback devotee. GMM #1218\\nDon't miss an all new Ear Biscuits: https://goo.gl/xeZNQt\\nWatch Part 4: https://youtu.be/MhCdiiB8CQg | Watch Part 2: https://youtu.be/7qiOrNao9fg\\nWatch today's episode from the start: http://bit.ly/GMM1218\\n\\nPick up all of the official GMM merch only at https://mythical.store\\n\\nFollow Rhett & Link: \\nInstagram: https://instagram.com/rhettandlink\\nFacebook: https://facebook.com/rhettandlink\\nTwitter: https://twitter.com/rhettandlink\\nTumblr: https://rhettandlink.tumblr.com\\nSnapchat: @realrhettlink\\nWebsite: https://mythical.co/\\n\\nCheck Out Our Other Mythical Channels:\\nGood Mythical MORE: https://youtube.com/goodmythicalmore\\nRhett & Link: https://youtube.com/rhettandlink\\nThis Is Mythical: https://youtube.com/thisismythical\\nEar Biscuits: https://applepodcasts.com/earbiscuits\\n\\nWant to send us something? https://mythical.co/contact\\nHave you made a Wheel of Mythicality intro video? Submit it here: https://bit.ly/GMMWheelIntro\\n\\nIntro Animation by Digital Twigs: https://www.digitaltwigs.com\\nIntro & Outro Music by Jeff Zeigler & Sarah Schimeneck https://www.jeffzeigler.com\\nWheel of Mythicality theme: https://www.royaltyfreemusiclibrary.com/\\nAll Supplemental Music fromOpus 1 Music: https://opus1.sourceaudio.com/\\nWe use ‘The Mouse’ by Blue Microphones https://www.bluemic.com/mouse/       |\n",
      "|d380meD0W0M|17.14.11     |I Dare You: GOING BALD!?                                      |nigahiga             |24         |2017-11-12 19:01:41|\"ryan\"|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"idy\"|\"rhpc\"|\"dares\"|\"no truth\"|\"comments\"|\"comedy\"|\"funny\"|\"stupid\"|\"fail\"                                                                                                                                                                                                                                                                                                                                                                     |2095731|132235|1989    |17518        |https://i.ytimg.com/vi/d380meD0W0M/default.jpg|False            |False           |False                 |I know it's been a while since we did this show, but we're back with what might be the best episode yet!\\nLeave your dares in the comment section! \\n\\nOrder my book how to write good \\nhttp://higatv.com/ryan-higas-how-to-write-good-pre-order-links/\\n\\nJust Launched New Official Store\\nhttps://www.gianthugs.com/collections/ryan\\n\\nHigaTV Channel\\nhttp://www.youtube.com/higatv\\n\\nTwitter\\nhttp://www.twitter.com/therealryanhiga\\n\\nFacebook\\nhttp://www.facebook.com/higatv\\n\\nWebsite\\nhttp://www.higatv.com\\n\\nInstagram\\nhttp://www.instagram.com/notryanhiga\\n\\nSend us mail or whatever you want here!\\nPO Box 232355\\nLas Vegas, NV 89105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+-----------+-------------+--------------------------------------------------------------+---------------------+-----------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+------+--------+-------------+----------------------------------------------+-----------------+----------------+----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. take()\n",
    "Va a tomas el número de filas que le digamos y nos las devuelve en forma de lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(video_id='2kyS6SvSYSE', trending_date='17.14.11', title='WE WANT TO TALK ABOUT OUR MARRIAGE', channel_title='CaseyNeistat', category_id='22', publish_time=datetime.datetime(2017, 11, 13, 18, 13, 1), tags='SHANtell martin', views=748374, likes=57527, dislikes=2966, comment_count=15954, thumbnail_link='https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg', comments_disabled='False', ratings_disabled='False', video_error_or_removed='False', description=\"SHANTELL'S CHANNEL - https://www.youtube.com/shantellmartin\\\\nCANDICE - https://www.lovebilly.com\\\\n\\\\nfilmed this video in 4k on this -- http://amzn.to/2sTDnRZ\\\\nwith this lens -- http://amzn.to/2rUJOmD\\\\nbig drone - http://tinyurl.com/h4ft3oy\\\\nOTHER GEAR ---  http://amzn.to/2o3GLX5\\\\nSony CAMERA http://amzn.to/2nOBmnv\\\\nOLD CAMERA; http://amzn.to/2o2cQBT\\\\nMAIN LENS; http://amzn.to/2od5gBJ\\\\nBIG SONY CAMERA; http://amzn.to/2nrdJRO\\\\nBIG Canon CAMERA; http://tinyurl.com/jn4q4vz\\\\nBENDY TRIPOD THING; http://tinyurl.com/gw3ylz2\\\\nYOU NEED THIS FOR THE BENDY TRIPOD; http://tinyurl.com/j8mzzua\\\\nWIDE LENS; http://tinyurl.com/jkfcm8t\\\\nMORE EXPENSIVE WIDE LENS; http://tinyurl.com/zrdgtou\\\\nSMALL CAMERA; http://tinyurl.com/hrrzhor\\\\nMICROPHONE; http://tinyurl.com/zefm4jy\\\\nOTHER MICROPHONE; http://tinyurl.com/jxgpj86\\\\nOLD DRONE (cheaper but still great);http://tinyurl.com/zcfmnmd\\\\n\\\\nfollow me; on http://instagram.com/caseyneistat\\\\non https://www.facebook.com/cneistat\\\\non https://twitter.com/CaseyNeistat\\\\n\\\\namazing intro song by https://soundcloud.com/discoteeth\\\\n\\\\nad disclosure.  THIS IS NOT AN AD.  not selling or promoting anything.  but samsung did produce the Shantell Video as a 'GALAXY PROJECT' which is an initiative that enables creators like Shantell and me to make projects we might otherwise not have the opportunity to make.  hope that's clear.  if not ask in the comments and i'll answer any specifics.\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfparquet3.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. head()\n",
    "Toma el número de registros que le indiquemos del comienzo del DF en forma de lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(video_id='2kyS6SvSYSE', trending_date='17.14.11', title='WE WANT TO TALK ABOUT OUR MARRIAGE', channel_title='CaseyNeistat', category_id='22', publish_time=datetime.datetime(2017, 11, 13, 18, 13, 1), tags='SHANtell martin', views=748374, likes=57527, dislikes=2966, comment_count=15954, thumbnail_link='https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg', comments_disabled='False', ratings_disabled='False', video_error_or_removed='False', description=\"SHANTELL'S CHANNEL - https://www.youtube.com/shantellmartin\\\\nCANDICE - https://www.lovebilly.com\\\\n\\\\nfilmed this video in 4k on this -- http://amzn.to/2sTDnRZ\\\\nwith this lens -- http://amzn.to/2rUJOmD\\\\nbig drone - http://tinyurl.com/h4ft3oy\\\\nOTHER GEAR ---  http://amzn.to/2o3GLX5\\\\nSony CAMERA http://amzn.to/2nOBmnv\\\\nOLD CAMERA; http://amzn.to/2o2cQBT\\\\nMAIN LENS; http://amzn.to/2od5gBJ\\\\nBIG SONY CAMERA; http://amzn.to/2nrdJRO\\\\nBIG Canon CAMERA; http://tinyurl.com/jn4q4vz\\\\nBENDY TRIPOD THING; http://tinyurl.com/gw3ylz2\\\\nYOU NEED THIS FOR THE BENDY TRIPOD; http://tinyurl.com/j8mzzua\\\\nWIDE LENS; http://tinyurl.com/jkfcm8t\\\\nMORE EXPENSIVE WIDE LENS; http://tinyurl.com/zrdgtou\\\\nSMALL CAMERA; http://tinyurl.com/hrrzhor\\\\nMICROPHONE; http://tinyurl.com/zefm4jy\\\\nOTHER MICROPHONE; http://tinyurl.com/jxgpj86\\\\nOLD DRONE (cheaper but still great);http://tinyurl.com/zcfmnmd\\\\n\\\\nfollow me; on http://instagram.com/caseyneistat\\\\non https://www.facebook.com/cneistat\\\\non https://twitter.com/CaseyNeistat\\\\n\\\\namazing intro song by https://soundcloud.com/discoteeth\\\\n\\\\nad disclosure.  THIS IS NOT AN AD.  not selling or promoting anything.  but samsung did produce the Shantell Video as a 'GALAXY PROJECT' which is an initiative that enables creators like Shantell and me to make projects we might otherwise not have the opportunity to make.  hope that's clear.  if not ask in the comments and i'll answer any specifics.\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfparquet3.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. collect()\n",
    "Nos trae todos los elementos que le indiquemos en forma de lista, pero hay que usarla con precaución porque si la aplicamos a una cantidad de datos demasiado grande, puede causar un error de desborde de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfparquet3.select('likes').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Escritura de DFs\n",
    "\n",
    "En sparkSQK la clase DataFrameWriter es la responsable de la lógica y la complejidad de escribir los datos de un DF en un sistema de almacenamiento externo.\\\n",
    "Una instancia de la clase DataFrameWriter está disponible como variable de escritura en la clase DataFrame.\\\n",
    "El patrón para interactuar con DataFrameWriter es similar al de DataFrameReader:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;df.write.format(...).mode(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).path(<path/to/file>)\n",
    "\n",
    "El formato predeterminado es .parquet\n",
    "\n",
    "Las funciones .partitionBy(...).bucketBy(...).sortBy(...) se utilizan para controlar la estructura de los directorios de salida en las fuentes de datos basadas en archivo.\n",
    "\n",
    "El modo de guardado controla cómo se manejará si la ruta de guardado ya existe y tiene contenido:\n",
    " * **append** agrega los datos del DF a la lista de archivos que ya existen en la ubicación de destino especificada\n",
    " * **overwrite** sobrescribe completamente cualquier archivo de datos que ya exista en la ubicación de destino especificada con los datos del DF.\n",
    " * **error, error if exist, default** es el modo por defecto. Si existe la ubicación de destino especificada DataFrameWriter arrojará un error.\n",
    " * **ignore** si existe la ubicación de destino especificada, simplemente no se hará nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear un DF a partir de dfparquet3, con un repartition\n",
    "\n",
    "df2 = dfparquet3.repartition(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lo guardamos cambiando el separador por defecto ',' por '|' y creará dos archivos porque el DF tiene dos particiones\n",
    "df2.write.format('csv').option('sep', '|').save('./output/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Si queremos que lo guarde en un solo csv, tenemos que dejar el DF en una sola partición\n",
    "df2.coalesce(1).write.format('csv').option('sep', '|').save('./output/csv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- trending_date: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- channel_title: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- publish_time: timestamp (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- views: integer (nullable = true)\n",
      " |-- likes: integer (nullable = true)\n",
      " |-- dislikes: integer (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- thumbnail_link: string (nullable = true)\n",
      " |-- comments_disabled: string (nullable = true)\n",
      " |-- ratings_disabled: string (nullable = true)\n",
      " |-- video_error_or_removed: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|comments_disabled|\n",
      "+-----------------+\n",
      "|            False|\n",
      "|             null|\n",
      "| sports and more.|\n",
      "|          Wiz Kid|\n",
      "|             True|\n",
      "|         farfalle|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparquet3.select('comments_disabled').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpio = dfparquet3.filter(col('comments_disabled').isin('True', 'False'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a particionar los datos por la columna 'comments_disabled' y guardarlo al mismo tiempo\n",
    "df_limpio.write.partitionBy('comments_disabled').parquet('./output/parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Persistencia de DFs\n",
    "\n",
    "Los DFs se pueden persistir o cachear en la memoria, tal como se hace con los RDD. Los mismos tipos de persistencia están disponibles en la clase de Data Frame.\\\n",
    "Sin embargo, hay una gran diferencia al almacenar en caché un data frame sparkSQL. Sabe el esquema de los datos dentro de un DF, por lo que organiza los datos en un formato de columnas y aplica las compresiones aplicables para minimizar el uso de espacio. Como resultado, se requerirá mucho menos espacio para almacenar un DF en la memoria que para almacenar una RDD cuando ambos están respaldados por el mismo archivo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'valor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|valor|\n",
      "+---+-----+\n",
      "|  1|    a|\n",
      "|  2|    b|\n",
      "|  3|    c|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, valor: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persistimos el DF en memoria\n",
    "df3.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, valor: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, valor: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con los RDD, podemos manejar el nivel del storage, el nivel donde queremos persistir nuestro DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, valor: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 11:50:53 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, valor: string]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, valor: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
