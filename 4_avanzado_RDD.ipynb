{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento en caché\n",
    "\n",
    "El almacenamiento en caché permite que Spark conserve los datos en todos los cálculos y operaciones.\\\n",
    "Es una de las técnicas más importantes de spark para acelerar los cálculos, especialmente cuando se trata de cálculos interactivos.\\\n",
    "Funciona almacenando el RDD tanto como sea posible en la memoria. Si los datos que se solicitan para almacenar en caché son más grandes que la memoria disponible, el rendimiento disminuirá porque se utilizará disco en lugar de memoria.\n",
    "\n",
    "* Podemos marcar un RDD como almacenado en caché usando **persist()** o **cache()**\n",
    "\n",
    "    * **cache()** es un sinónimo de persist(MEMORY_ONLY)\n",
    "    * **persist()** puede usar memoria, disco o ambos\n",
    "\n",
    "#### Valores posibles para el nivel de almacenamiento\n",
    "\n",
    "* **MEMORY_ONLY** almacena el RDD como un objeto Java deserialización en la JVM. Si el RDD no cabe memoria, algunas particiones no se almacenarán en caché y se volverán a calcular sobre la marcha cada vez que se necesiten. Este es el nivel por defecto en spark.\n",
    "\n",
    "* **MEMORY_AND_DISK** almacena los RDD como objetos Java deserializados en la JVM. Si el RDD no cabe en memoria, almacena las particiones que no quepan en el disco y las lee desde allí cuando sea necesario.\n",
    "\n",
    "* **DISK_ONLY** almacena las particiones del RD solo en disco.\n",
    "\n",
    "* **MEMORY_ONLY_2, MEORY_AND_DISK_2, etc** igual que los niveles anteriores, pero replica cada partición en los nodos del cluster.\n",
    "\n",
    "#### ¿Qué nivel de almacenamiento elegir?\n",
    "El nivel de almacenamiento a elegir depende de la situación:\n",
    "\n",
    "* Si los RDD caben en la memoria, use **MEMORY_ONLY**, ya que es la opción más rápida para el rendimiento de ejecución.\n",
    "\n",
    "* **DISK_ONLY** no debe usarse a menos que sus cálculos sean costosos.\n",
    "\n",
    "*  Utilice almacenamiento replicado para una mejor tolerancia fallos si puede ahorrar la memoria adicional necesaria. Esto evitará que se vuelvan a calcular las particiones perdidas para obtener la mejor disponibilidad.\n",
    "\n",
    ">Podemos usar la función **unpersist()** para liberar el contenido en cahé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([item for item in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Si quisiéramos cambiar el nivel de persistencia al mismo RDD, debemos tener en cuenta que primero es necesario hacerle **unpersist()** a este mismo RDD. De lo contrario, si tratamos de cambiarle el nivel de persistencia sin realizar esta acción, nos va a devolver un error.\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.unpersist()\n",
    "\n",
    "rdd.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.unpersist()\n",
    "# Cuando le aplicamos cache(), estamos haciendo un MEMORY_ONLY\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition y shuffling (mezcla de datos)\n",
    "\n",
    "* Los RDD operan con datos no como una sola masa de datos, sino que administran y operan los datos en particiones repartidas por todo el cluster. Por lo tanto, el concepto de partición de datos es fundamental para el correcto funcionamiento de los chop de Apache spark y puede tener un gran efecto en el rendimiento y en la forma en que se utilizan los recursos.\n",
    "\n",
    "* Los RDD constan de particiones de datos y todas las operaciones se realizan en las particiones de datos en el RDD. Algunas operaciones, como las **transformaciones**, son funciones ejecutadas por un ejecutor en la partición específica de datos en la que se opera. Sin embargo, no todas las operaciones pueden realizarse simplemente realizando operaciones aisladas en las particiones de datos por parte de los respectivos ejecutores Las operaciones como las **agregaciones**, requieren que los datos se muevan a través del cluster en una fase conocida como mezcla o **shuffle**.\n",
    "\n",
    "#### Importancia del número de particiones\n",
    "\n",
    "* Si la cantidad de particiones es demasiado pequeña, usaremos sólo unas pocas CPU o núcleos en una gran cantidad de datos, por lo que tendremos un rendimiento más lento y dejaremos el cluster subutilizado.\n",
    "\n",
    "* Si la cantidad de particiones es demasiado grande, utilizará más recursos de los que realmente necesita y en un entorno de múltiples procesos, podría estar provocando la falta de recursos para otros procesos que usted u otros miembros de su equipo ejecutan.\n",
    "\n",
    "#### Particionadores\n",
    "\n",
    "El particionamiento de los RDD se realiza mediante patinadores. Los patinadores asignan un índice de partición a los elementos del RDD. Todos los elementos de la misma partición tendrán el mismo índice de partición.\\\n",
    "Spark dispone de dos patinadores el HashPartitioner y el RangePartitioner.\n",
    "Además de esto, también puede implementar un patrocinador personalizado.\n",
    "\n",
    "* **HashPartitioner:** es el patrocinador predeterminado en spark y funciona calculando un valor hash para cada clave de los elementos. Todos los elementos con el mismo código hash terminan en una misma partición y esto se hace utilizando la fórmula siguiente:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;partitionIndex = hash(key) % numPartitions\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;El partitionIndex es iguel al hash del elemento dividido entre el número de particiones\n",
    "\n",
    "* **RangePartitioner:** funciona dividiendo el RDD en rangos aproximadamente iguales. Dado que el rango debe conocer las claves de inicio y final de cualquier partición, el RDD debe ordenarse primero antes de que se pueda usar un RangePartitioner. RangePartitioner primero necesita límites razonables para las particiones basadas en el RDD, luego crea una función desde la clave K hasta el partitionIndex al que pertenece el elemento y finalmente necesitamos reparticionar el RDD, basado en el RangePartitioner y para redistribuir los elementos del RDD correctamente según los rangos que determinamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Veamos en la práctica cómo podemos implementar un **HashPartitioner** manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize(['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hola = 'Hola'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514654146388779058"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos el valor hash de la variable hola\n",
    "hash(hola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particiones = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# indice = hash(item) % num_particiones\n",
    "\n",
    "print(hash('x') % num_particiones)\n",
    "\n",
    "print(hash('y') % num_particiones)\n",
    "\n",
    "print(hash('z') % num_particiones)\n",
    "\n",
    "# Aquí podemos ver a qué partición irá cada valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling (mezcla de datos)\n",
    "\n",
    "Cualquiera que sea el participio utilizado, muchas operaciones provocarán un reparticionamiento de datos en las particiones de un RDD. Se pueden crear nuevas particiones o se pueden contraer o fusionar varias particiones.\n",
    "\n",
    "Todo el movimiento de datos necesario para el reparto se denomina **shuffling** y este es un concepto importante que hay que comprender al escribir un job de Spark.\n",
    "\n",
    "El shuffling puede causar un gran impacto en el rendimiento, ya que los cálculos ya no están en la memoria del mismo ejecutor, sino que los ejecutores están intercambiando datos a través de la red.\n",
    "\n",
    "Cuando un RDD está experimentando una transformación, se intenta que las operaciones que se realicen en el mismo nodo que los datos. Sin embargo, a menudo utilizamos operaciones de unión, reducción, agrupación o agregación, entre otras, que provocan el shuffling intencionado o no intencionado.\n",
    "\n",
    "Este shuffle, a su vez, determina dónde ha finalizado una etapa particular del procesamiento y donde ha comenzado una nueva.\n",
    "\n",
    ">\\\n",
    ">Cuanto más suffling tengamos, más etapas o stages ocurren en la ejecución del Job que afecta el rendimiento. Por lo tanto, estos temas debemos tenerlos en cuenta a la hora de construir de nuestros Jobs de Spark.\n",
    ">\n",
    "><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast variables\n",
    "\n",
    "Las variables broadcast son variables compartidas entre todos los ejecutores. Éstas se crean una vez en el controlador y luego se leen sólo en los ejecutores.\n",
    "\n",
    "Se pueden transmitir conjuntos de datos completos en un cluster de Spark para que los ejecutores tengan acceso a los datos transmitidos. Todas las tareas que se ejecutan dentro de un ejecutor tienen acceso a las variables broadcast.\n",
    "\n",
    "Lo que sucede al crear una variable broadcast es que la información de esta variable estará disponible en todos los **worker nodes** y será posible acceder a los datos de esta misma variable broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uno = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una variable broadcast llamando a la función broadcast del sparl context\n",
    "br_uno = sc.broadcast(uno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a sumar al primer rdd que creamos el valor de la variable broadcast, llamándola con .value\n",
    "rdd3 = rdd.map(lambda x: x + br_uno.value)\n",
    "\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Las variables broadcast ocupan memoria en todos los ejecutores y según el tamaño de los datos contenidos en la variable broadcast, estos puede causar problemas de recursos en algún momento.\n",
    ">\n",
    "><br>\n",
    "\n",
    "Existe una forma de eliminar las variables de la memoria de todos los ejecutores.\n",
    "Para ello, lo que vamos a hacer es llamar a la función **unpersist()** en una variable que previamente lavtengamos como broadcast y esto lo que va a hacer es eliminar los datos de la variable broadcast de la memoria de caché de todos los ejecutores para así liberar recursos.\n",
    "\n",
    "Si la variable se volviera a utilizar los datos se retransmitirían a los ejecutores para que se vuelvan a utilizar.\n",
    "\n",
    "Tenemos que tener en cuenta que después de llamar a unpersist(), si accedemos a la variable broadcast nuevamente funciona como de costumbre, es decir, por detrás de escena los executors están extrayendo los datos de la variable nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al aplicar unpersist() retiramos la variable de los executors\n",
    "br_uno.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si volvemos a llamar a la broadcast variable a la que le acabamos de aplicar el unpersist(), se ejecuta sin ningún tipo de problema\n",
    "rdd4 = rdd.map(lambda x: x + br_uno.value)\n",
    "\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\\\n",
    ">Para eliminar completamente una variable broadcast, hay que eliminarla de todos los executors y del driver con la función **destroy()**\n",
    ">\n",
    ">Esto puede resultar muy útil para administrar los recursos de manera óptima en todo el cluster.\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destroy()  destruye todos los datos y metadatos relacionados con la variable broadcast especificada.\n",
    "br_uno.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si ahora intentamos volver a acceder a la variable broadcast, nos arrojará un error\n",
    "rdd5  = rdd.map(lambda x: x + br_uno.value)\n",
    "\n",
    "rdd5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulator variables\n",
    "\n",
    "Los acumuladores son variables compartidas entre los ejecutores, que normalmente se utilizan para agregar contadores a su programa en Spark.\n",
    "\n",
    "* **sparkContext.accumulator()** se usa para definir variables de acumulador\n",
    "\n",
    "* **add()** se usa para agregar/actualizar un valor en el acumulador\n",
    "\n",
    "* La propiedad **.value** de la variable del acumulador se utiliza para recuperar el valor del acumulador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero creamos un accumulator dándole como parámetro el valor con el cual deseamos iniciar el acumulador\n",
    "acumulador = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd6 = sc.parallelize([2,4,6,8,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a calcular suma total de los elementos que componen el RDD usando el accumulator\n",
    "# Lo que vamos a hacer es ir sumando al accumulator cada uno de los valores del RDD\n",
    "rdd6.foreach(lambda x: acumulador.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos el valor del accumulator\n",
    "print(acumulador.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# También podemos usar accumulator para saber la cantidad de elementos que contiene el RDD\n",
    "rdd1 = sc.parallelize('Mi nombre es Miguel y me siento genial'.split(' '))\n",
    "\n",
    "acumulador1 = sc.accumulator(0)\n",
    "\n",
    "rdd1.foreach(lambda x: acumulador1.add(1))\n",
    "\n",
    "print(acumulador1.value)\n",
    "# Y 8 será la cantidad de palabras contenidas en la frase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-IyzVi8nU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
